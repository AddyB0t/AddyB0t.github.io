<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Addy</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Addy</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Dec 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Resources to keep up with AI</title>
      <link>http://localhost:1313/posts/resources-to-keep-up-with-ai/</link>
      <pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/resources-to-keep-up-with-ai/</guid>
      <description>&lt;p&gt;I have not listed the resources in any particular order(say these are the resources for keeping update with research, these are the resources for latest news, etc). The resources are listed paltform wise. These resources are the best resources to keep updated with research, controversies, latest news, etc all rolled into one place(That is how we keep the things intresting)&lt;/p&gt;
&lt;h1 id=&#34;youtube&#34;&gt;Youtube&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@1littlecoder&#34;&gt;https://www.youtube.com/@1littlecoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@3blue1brown&#34;&gt;https://www.youtube.com/@3blue1brown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@WesRoth&#34;&gt;https://www.youtube.com/@WesRoth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AICoffeeBreak&#34;&gt;https://www.youtube.com/@AICoffeeBreak&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@misraturp&#34;&gt;https://www.youtube.com/@misraturp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@intheworldofai&#34;&gt;https://www.youtube.com/@intheworldofai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheoreticallyMedia&#34;&gt;https://www.youtube.com/@TheoreticallyMedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TwoMinutePapers&#34;&gt;https://www.youtube.com/@TwoMinutePapers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@VoloBuilds&#34;&gt;https://www.youtube.com/@VoloBuilds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MervinPraison&#34;&gt;https://www.youtube.com/@MervinPraison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@WelchLabsVideo&#34;&gt;https://www.youtube.com/@WelchLabsVideo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Deeplearningai&#34;&gt;https://www.youtube.com/@Deeplearningai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@underfitted&#34;&gt;https://www.youtube.com/@underfitted&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@matthew_berman&#34;&gt;https://www.youtube.com/@matthew_berman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheAiGrid&#34;&gt;https://www.youtube.com/@TheAiGrid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@visualkernel&#34;&gt;https://www.youtube.com/@visualkernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@t3dotgg&#34;&gt;https://www.youtube.com/@t3dotgg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SkillLeapAI&#34;&gt;https://www.youtube.com/@SkillLeapAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SebastianRaschka&#34;&gt;https://www.youtube.com/@SebastianRaschka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@VisuallyExplained&#34;&gt;https://www.youtube.com/@VisuallyExplained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MattVidPro&#34;&gt;https://www.youtube.com/@MattVidPro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@WhatsAI&#34;&gt;https://www.youtube.com/@WhatsAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ritvikmath&#34;&gt;https://www.youtube.com/@ritvikmath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@theorants&#34;&gt;https://www.youtube.com/@theorants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@engineerprompt&#34;&gt;https://www.youtube.com/@engineerprompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@aiadvantage&#34;&gt;https://www.youtube.com/@aiadvantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DavidOndrej&#34;&gt;https://www.youtube.com/@DavidOndrej&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@umarjamilai&#34;&gt;https://www.youtube.com/@umarjamilai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheoThrowaways&#34;&gt;https://www.youtube.com/@TheoThrowaways&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@OpenAI&#34;&gt;https://www.youtube.com/@OpenAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@OlivioSarikas&#34;&gt;https://www.youtube.com/@OlivioSarikas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@rileybrownai&#34;&gt;https://www.youtube.com/@rileybrownai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@replit&#34;&gt;https://www.youtube.com/@replit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@outliier&#34;&gt;https://www.youtube.com/@outliier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@NeuralNine&#34;&gt;https://www.youtube.com/@NeuralNine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@szymonozog7862&#34;&gt;https://www.youtube.com/@szymonozog7862&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SirajRaval&#34;&gt;https://www.youtube.com/@SirajRaval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@sedetweiler&#34;&gt;https://www.youtube.com/@sedetweiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@avb_fj&#34;&gt;https://www.youtube.com/@avb_fj&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@what-is-math&#34;&gt;https://www.youtube.com/@what-is-math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@replicatehq&#34;&gt;https://www.youtube.com/@replicatehq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@aiexplained-official&#34;&gt;https://www.youtube.com/@aiexplained-official&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AIJasonZ&#34;&gt;https://www.youtube.com/@AIJasonZ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@dlByManish&#34;&gt;https://www.youtube.com/@dlByManish&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@mreflow&#34;&gt;https://www.youtube.com/@mreflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@neuralink&#34;&gt;https://www.youtube.com/@neuralink&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@FlowiseAI&#34;&gt;https://www.youtube.com/@FlowiseAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@NicholasRenotte&#34;&gt;https://www.youtube.com/@NicholasRenotte&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@OnyxStudiosInteractive&#34;&gt;https://www.youtube.com/@OnyxStudiosInteractive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@YannicKilcher&#34;&gt;https://www.youtube.com/@YannicKilcher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@RobertMilesAI&#34;&gt;https://www.youtube.com/@RobertMilesAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@jamesbriggs&#34;&gt;https://www.youtube.com/@jamesbriggs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@rowanch&#34;&gt;https://www.youtube.com/@rowanch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MachineLearningWithJay&#34;&gt;https://www.youtube.com/@MachineLearningWithJay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@unitreerobotics&#34;&gt;https://www.youtube.com/@unitreerobotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@VercelHQ&#34;&gt;https://www.youtube.com/@VercelHQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@qilinxue989&#34;&gt;https://www.youtube.com/@qilinxue989&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@kishalaydas&#34;&gt;https://www.youtube.com/@kishalaydas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@bycloudAI&#34;&gt;https://www.youtube.com/@bycloudAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@elvissaravia&#34;&gt;https://www.youtube.com/@elvissaravia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@YTomS&#34;&gt;https://www.youtube.com/@YTomS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/sciencephiletheai&#34;&gt;https://www.youtube.com/sciencephiletheai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MachineLearningSimulation&#34;&gt;https://www.youtube.com/@MachineLearningSimulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@EdenMarco&#34;&gt;https://www.youtube.com/@EdenMarco&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Fireship&#34;&gt;https://www.youtube.com/@Fireship&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Tech-Rogue&#34;&gt;https://www.youtube.com/@Tech-Rogue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SimonHoiberg&#34;&gt;https://www.youtube.com/@SimonHoiberg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@PixelArtistry&#34;&gt;https://www.youtube.com/@PixelArtistry&lt;/a&gt;_&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@leonvanzyl&#34;&gt;https://www.youtube.com/@leonvanzyl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@soroushmehraban&#34;&gt;https://www.youtube.com/@soroushmehraban&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@prathoshap5226&#34;&gt;https://www.youtube.com/@prathoshap5226&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@simplyexplained&#34;&gt;https://www.youtube.com/@simplyexplained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TechWithTim&#34;&gt;https://www.youtube.com/@TechWithTim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@howardjeremyp&#34;&gt;https://www.youtube.com/@howardjeremyp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@NormalizedNerd&#34;&gt;https://www.youtube.com/@NormalizedNerd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ShusenWangEng&#34;&gt;https://www.youtube.com/@ShusenWangEng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheFutureThinker&#34;&gt;https://www.youtube.com/@TheFutureThinker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@code4AI&#34;&gt;https://www.youtube.com/@code4AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@eyeonai3425&#34;&gt;https://www.youtube.com/@eyeonai3425&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@robmulla&#34;&gt;https://www.youtube.com/@robmulla&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@figureai&#34;&gt;https://www.youtube.com/@figureai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@HuggingFace&#34;&gt;https://www.youtube.com/@HuggingFace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SmithaKolan&#34;&gt;https://www.youtube.com/@SmithaKolan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Langflow&#34;&gt;https://www.youtube.com/@Langflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@animatedai&#34;&gt;https://www.youtube.com/@animatedai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheFocusedCoder&#34;&gt;https://www.youtube.com/@TheFocusedCoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@echohive&#34;&gt;https://www.youtube.com/@echohive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@BostonDynamics&#34;&gt;https://www.youtube.com/@BostonDynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@lexfridman&#34;&gt;https://www.youtube.com/@lexfridman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SerranoAcademy&#34;&gt;https://www.youtube.com/@SerranoAcademy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@LaCarnevali&#34;&gt;https://www.youtube.com/@LaCarnevali&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@campusx-official&#34;&gt;https://www.youtube.com/@campusx-official&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MicroConf&#34;&gt;https://www.youtube.com/@MicroConf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@anthropic-ai&#34;&gt;https://www.youtube.com/@anthropic-ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@MachineLearningStreetTalk&#34;&gt;https://www.youtube.com/@MachineLearningStreetTalk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@jbhuang0604&#34;&gt;https://www.youtube.com/@jbhuang0604&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@LangChain&#34;&gt;https://www.youtube.com/@LangChain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@joshfpocock&#34;&gt;https://www.youtube.com/@joshfpocock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@CodeEmporium&#34;&gt;https://www.youtube.com/@CodeEmporium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@deepbean&#34;&gt;https://www.youtube.com/@deepbean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@RobShocks&#34;&gt;https://www.youtube.com/@RobShocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@digitallearninghub-imperia3540&#34;&gt;https://www.youtube.com/@digitallearninghub-imperia3540&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@aipapersacademy&#34;&gt;https://www.youtube.com/@aipapersacademy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@FlowiseAI&#34;&gt;https://www.youtube.com/@FlowiseAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DevelopersDigest&#34;&gt;https://www.youtube.com/@DevelopersDigest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ThePyCoach&#34;&gt;https://www.youtube.com/@ThePyCoach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AndrejKarpathy&#34;&gt;https://www.youtube.com/@AndrejKarpathy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@lethal-intelligence&#34;&gt;https://www.youtube.com/@lethal-intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@BobDoyleMedia&#34;&gt;https://www.youtube.com/@BobDoyleMedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@KapilSachdeva&#34;&gt;https://www.youtube.com/@KapilSachdeva&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@SamuelAlbanie1&#34;&gt;https://www.youtube.com/@SamuelAlbanie1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@arp_ai&#34;&gt;https://www.youtube.com/@arp_ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ariseffai&#34;&gt;https://www.youtube.com/@ariseffai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@LevendeStreg&#34;&gt;https://www.youtube.com/@LevendeStreg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DreamingAIChannel&#34;&gt;https://www.youtube.com/@DreamingAIChannel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@indydevdan&#34;&gt;https://www.youtube.com/@indydevdan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@theAIsearch&#34;&gt;https://www.youtube.com/@theAIsearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@algorithmicsimplicity&#34;&gt;https://www.youtube.com/@algorithmicsimplicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Aitrepreneur&#34;&gt;https://www.youtube.com/@Aitrepreneur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DevelopersHutt&#34;&gt;https://www.youtube.com/@DevelopersHutt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ArtOfTheProblem&#34;&gt;https://www.youtube.com/@ArtOfTheProblem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@IntuitiveMachineLearning&#34;&gt;https://www.youtube.com/@IntuitiveMachineLearning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@GregIsenberg&#34;&gt;https://www.youtube.com/@GregIsenberg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@CNETHighlights&#34;&gt;https://www.youtube.com/@CNETHighlights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@bhancock_ai&#34;&gt;https://www.youtube.com/@bhancock_ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DwarkeshPatel&#34;&gt;https://www.youtube.com/@DwarkeshPatel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@OptimisticFuturology&#34;&gt;https://www.youtube.com/@OptimisticFuturology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@hu-po&#34;&gt;https://www.youtube.com/@hu-po&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@daveebbelaar&#34;&gt;https://www.youtube.com/@daveebbelaar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@controlaltai&#34;&gt;https://www.youtube.com/@controlaltai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Explaining-AI&#34;&gt;https://www.youtube.com/@Explaining-AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@lewingtonn&#34;&gt;https://www.youtube.com/@lewingtonn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@donatocapitella&#34;&gt;https://www.youtube.com/@donatocapitella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DenisDmitrievDeepRobotics&#34;&gt;https://www.youtube.com/@DenisDmitrievDeepRobotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@BryanJohnson&#34;&gt;https://www.youtube.com/@BryanJohnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@EmergentGarden&#34;&gt;https://www.youtube.com/@EmergentGarden&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@glebkrasmedia&#34;&gt;https://www.youtube.com/@glebkrasmedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DrTrefor&#34;&gt;https://www.youtube.com/@DrTrefor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AIBites&#34;&gt;https://www.youtube.com/@AIBites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@FuturMinds&#34;&gt;https://www.youtube.com/@FuturMinds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AssemblyAI&#34;&gt;https://www.youtube.com/@AssemblyAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Computerphile&#34;&gt;https://www.youtube.com/@Computerphile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Green-Code&#34;&gt;https://www.youtube.com/@Green-Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ArxivInsights&#34;&gt;https://www.youtube.com/@ArxivInsights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@CreatorMagicAI&#34;&gt;https://www.youtube.com/@CreatorMagicAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@gautamiruvanti7592&#34;&gt;https://www.youtube.com/@gautamiruvanti7592&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AAmini&#34;&gt;https://www.youtube.com/@AAmini&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@HeduAI&#34;&gt;https://www.youtube.com/@HeduAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AIBeyondInfinity&#34;&gt;https://www.youtube.com/@AIBeyondInfinity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@gabrielmongaras&#34;&gt;https://www.youtube.com/@gabrielmongaras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AI.Tooltip&#34;&gt;https://www.youtube.com/@AI.Tooltip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@CodingwithGPT&#34;&gt;https://www.youtube.com/@CodingwithGPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@calendarG&#34;&gt;https://www.youtube.com/@calendarG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@CNET&#34;&gt;https://www.youtube.com/@CNET&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@ariseffai&#34;&gt;https://www.youtube.com/@ariseffai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@beyondfireship&#34;&gt;https://www.youtube.com/@beyondfireship&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@GoogleQuantumAI&#34;&gt;https://www.youtube.com/@GoogleQuantumAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DeepFindr&#34;&gt;https://www.youtube.com/@DeepFindr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@TheAIEpiphany&#34;&gt;https://www.youtube.com/@TheAIEpiphany&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@chatwithdata&#34;&gt;https://www.youtube.com/@chatwithdata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AdamLucek&#34;&gt;https://www.youtube.com/@AdamLucek&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@borismeinardus&#34;&gt;https://www.youtube.com/@borismeinardus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@edrickdch&#34;&gt;https://www.youtube.com/@edrickdch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AIAnimationStudio&#34;&gt;https://www.youtube.com/@AIAnimationStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@menloparklab&#34;&gt;https://www.youtube.com/@menloparklab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@henrikkniberg&#34;&gt;https://www.youtube.com/@henrikkniberg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Gonkee&#34;&gt;https://www.youtube.com/@Gonkee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@DubStepZz&#34;&gt;https://www.youtube.com/@DubStepZz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@allin&#34;&gt;https://www.youtube.com/@allin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@4tuneGuide_official&#34;&gt;https://www.youtube.com/@4tuneGuide_official&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AISciencesLearn&#34;&gt;https://www.youtube.com/@AISciencesLearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@AladdinPersson&#34;&gt;https://www.youtube.com/@AladdinPersson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@Augmented_AI&#34;&gt;https://www.youtube.com/@Augmented_AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>16 Jan 2025</title>
      <link>http://localhost:1313/posts/reinforcement-learning/16-jan-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/16-jan-2025/</guid>
      <description>&lt;h2 id=&#34;controlled-markov-chain-cmc&#34;&gt;Controlled Markov Chain (CMC)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A Controlled Markov Chain (CMC) is a stochastic process ${X_n}_{n \geq 0}$ that evolves over discrete time steps $n = 0, 1, 2, \ldots$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At each time step $n$, the system is in a state $X_n$ belonging to a state space $S$. To influence the evolution of this process, we apply a control or action $Z_n$ from a set of feasible actions $A(X_n)$ that depends on the current state $X_n$. Thus, $Z_n \in A(X_n)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaboost</title>
      <link>http://localhost:1313/posts/prnn/33--adaboost/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/33--adaboost/</guid>
      <description>&lt;h2 id=&#34;recall&#34;&gt;Recall&lt;/h2&gt;
&lt;p&gt;Recall from our discussion on boosting we defined the ensemble model $H_T(x)$ as:&lt;/p&gt;
&lt;p&gt;$$ H_T(x) = \sum_{t=1}^T \alpha_t h_t(x) $$&lt;/p&gt;
&lt;p&gt;the problem of finding the next weak learner reduces to:&lt;/p&gt;
&lt;p&gt;$$ h_T = \arg\min_{h \in H} \sum_{i=1}^n r_i \cdot h(x_i) $$&lt;/p&gt;
&lt;p&gt;where
$$r_i = -\frac{\partial R(H_{T-1}(x_i))}{\partial H_{T-1}(x_i)}$$&lt;/p&gt;
&lt;h2 id=&#34;problem-setting&#34;&gt;Problem setting&lt;/h2&gt;
&lt;p&gt;Let the dataset be $D = {(x_i, y_i)}_{i=1}^n$ where $y_i \in {-1, 1}$.&lt;/p&gt;
&lt;p&gt;The loss function is given by:&lt;/p&gt;
&lt;p&gt;$$L(H(x_i), y_i) = \exp(-y_i H(x_i))$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aggregated posterior mismatch</title>
      <link>http://localhost:1313/posts/adrl/13---aggregated-posterior-mismatch-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/13---aggregated-posterior-mismatch-/</guid>
      <description>&lt;h2 id=&#34;why-did-we-not-have-such-a-problem-in-em-algorithm&#34;&gt;Why did we not have such a problem in EM algorithm?&lt;/h2&gt;
&lt;p&gt;The Variational Autoencoder (VAE) architecture can be visualized as follows:&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/24.JPG&#34; alt=&#34;Variational Autoencoder Architecture&#34; width=&#34;900&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;
&lt;p&gt;Before expanding the KL divergence term, recall that:&lt;/p&gt;
&lt;p&gt;latent variable $q_{\phi}(z|x)$ is the approximate posterior (i.e., how likely the latent variable $z$ is given the input $x$).
$p_{\theta}(z|x)$ is the true posterior (i.e., the actual distribution of $z$ given $x$ under the model).
The KL divergence measures how much $q_{\phi}(z|x)$ deviates from $p_{\theta}(z|x)$:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Roadmap</title>
      <link>http://localhost:1313/posts/ai-roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/ai-roadmap/</guid>
      <description>&lt;h1 id=&#34;ai-learning-roadmap&#34;&gt;AI Learning Roadmap&lt;/h1&gt;
&lt;p&gt;I have made this roadmap that I will give to my younger brother who is starting his learning journey in AI. The key points of this roadmap are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The roadmap assumes you have basic knowledge of maths and programming of a 12th class pass out.&lt;/li&gt;
&lt;li&gt;All the resources are free.&lt;/li&gt;
&lt;li&gt;The roadmap is linear, follow it step by step. However, if you have a good grasp of a topic, you can skip it and move to the immediate next topic.&lt;/li&gt;
&lt;li&gt;The roadmap is not exhaustive, I have designed it so that you learn 90% of the most important topics in AI.&lt;/li&gt;
&lt;li&gt;I have seen most AI learning resources to either focus on maths or programming. I have designed this roadmap to cover both.&lt;/li&gt;
&lt;li&gt;This is not the only possible roadmap, there are many other roadmaps that are possible. This is just a roadmap that I think is best for a beginner learning on his own.&lt;/li&gt;
&lt;li&gt;There are thousands of students from all walks of life studying this roadmap in this &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Discord&lt;/a&gt; server&lt;/li&gt;
&lt;li&gt;I have made the &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Discord&lt;/a&gt; server for students who are following this roadmap to help them learn and grow together. Here is the link to the server: &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Join our Discord Server&lt;/a&gt;. Do join thousands of students who would be learning along with you.&lt;/li&gt;
&lt;li&gt;I also encourage you to learn about the topics from other sources as well in addition to the ones mentioned in the roadmap if you have more time.&lt;/li&gt;
&lt;li&gt;In case you are not able to understand a particular topic from the resources mentioned in the roadmap, do explore other resources and share your experience.&lt;/li&gt;
&lt;li&gt;As and when I find a better new resource, I will update the roadmap. If you have any suggestions, please let me know so that I can update the roadmap.&lt;/li&gt;
&lt;li&gt;This roadmap is up to date. As and when an important breakthrough comes in the field of AI, that is important, I will update the roadmap.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;after-completing-the-roadmap&#34;&gt;After completing the roadmap:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You should pick one or more area of expertise like Computer Vision, Natural Language Processing, etc. and start applying your knowledge and learning advance topics in that domain.&lt;/li&gt;
&lt;li&gt;There is no use of learning AI if you are not applying your knowledge. You should start working on real world projects. Either start working on your own projects or contribute to the projects at your company/open source community.&lt;/li&gt;
&lt;li&gt;AI is one of the fastest growing field, so do keep yourself updated with the latest research, news and products.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;my-tips-for-the-discord-server&#34;&gt;My tips for the &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Discord&lt;/a&gt; server:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Discord&lt;/a&gt; server is free and open to all with a learning mindset. Do invite your friends and classmates by sharing the server link on your social media handles and bring them along on this learning journey so that you can learn and grow together. Moreover the more members in the server, the more easier it will be to get help from each other and more resources will be shared for everyone&amp;rsquo;s benefit.&lt;/li&gt;
&lt;li&gt;Do not hesitate to ask questions. Moreover, if you have time do help others by answering their questions as this will help you solidify your understanding of the topic.&lt;/li&gt;
&lt;li&gt;Do find friends in this &lt;a href=&#34;https://discord.com/invite/QHAbprqQme&#34;&gt;Discord&lt;/a&gt; server who are learning along with you and do form a study group(if possible find friends that you can even meet in person).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;here-is-the-roadmap&#34;&gt;Here is the roadmap:&lt;/h2&gt;
&lt;h3 id=&#34;1-maths-prerequisites&#34;&gt;1. Maths prerequisites&lt;/h3&gt;
&lt;h4 id=&#34;11-linear-algebra&#34;&gt;1.1 Linear Algebra&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLiiljHvN6z1_o1ztXTKWPrShrMrBLo5P3&#34;&gt;Linear Algebra by Imperial College London&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&#34;&gt;3Blue1Brown&amp;rsquo;s &amp;ldquo;Essence of Linear Algebra&amp;rdquo; playlist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-calculus-and-optimization&#34;&gt;1.2 Calculus and Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLiiljHvN6z193BBzS0Ln8NnqQmzimTW23&#34;&gt;Multivariate Calculus by Imperial College London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;13-probability&#34;&gt;1.3 Probability&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLBh2i93oe2qswFOC98oSFc37-0f4S3D4z&#34;&gt;Probability Theory from The Bright Side of Mathematics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-coding&#34;&gt;2. Coding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kqtD5dpn9C8&#34;&gt;Python for Beginners by Mosh&lt;/a&gt; (Skip if you already know basics of Python)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8JJ101D3knE&#34;&gt;Learn Git from Mosh&lt;/a&gt; (Skip if you already know Git)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KxvKCSwlUv8&#34;&gt;Learn to make virtual environment in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN&#34;&gt;Pytorch beginner series by PyTorch&lt;/a&gt; (Do not try to understand everything about PyTorch at once, just get some idea then learn more about it as you code AI models from Machine Learning and Deep Learning section)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-machine-learning&#34;&gt;3. Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&#34;&gt;Machine Learning by Josh Starmer from StatQuest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do implement all the basic machine learning models like Linear Regression, Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forest, Gradient Boosting, etc. that are taught in the playlist in PyTorch. Youtube has a lot of tutorials on how to implement these models in PyTorch.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-deep-learning&#34;&gt;4. Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&#34;&gt;Deep Learning by Josh Starmer from StatQuest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&#34;&gt;Neural networks from 3Blue1Brown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implement all the models learned in the playlists in PyTorch. You can follow &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&#34;&gt;this playlist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Become proficient in coding Deep Learning models by following &lt;a href=&#34;https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&#34;&gt;Andrej Karpathy&amp;rsquo;s Zero to Hero series&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally I will again emphasize that this is not the end of the learning journey, rather it is the beginning. I have designed this roadmap so that you learn 90% of the most important topics in AI which will act as a foundation for your further learning a particular subdomain of AI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anderson&#39;s result</title>
      <link>http://localhost:1313/posts/adrl/26.2-andersons-result/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/26.2-andersons-result/</guid>
      <description>&lt;p&gt;Anderson&amp;rsquo;s result states that for a forward SDE of the form:&lt;/p&gt;
&lt;p&gt;$$dx = f(x,t)dt + g(t)dB_t$$&lt;/p&gt;
&lt;p&gt;The corresponding reverse SDE takes the form:&lt;/p&gt;
&lt;p&gt;$$dx = \left(f(x,t) - g^2(t)\nabla_x(\log p_t(x))\right)dt + g(t)dB_t$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f(x,t)$ is the drift term of the forward process&lt;/li&gt;
&lt;li&gt;$g(t)$ is the diffusion coefficient&lt;/li&gt;
&lt;li&gt;$\nabla_x(\log p_t(x))$ is the score function&lt;/li&gt;
&lt;li&gt;The reverse process has the same diffusion term $g(t)dB_t$ as the forward process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This result shows that to reverse a diffusion process, we need to:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Andrew Tate</title>
      <link>http://localhost:1313/posts/andrew-tate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/andrew-tate/</guid>
      <description>&lt;p&gt;These are some things that I have learned from watching Andrew Tate&amp;rsquo;s videos:&lt;/p&gt;
&lt;h2 id=&#34;politicians&#34;&gt;Politicians&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Politicians are not stupid&lt;/strong&gt;: Do not think that politicians are stupid; that is a very optimistic view. They are not stupid; they know what they are doing. It is you who is confused because you think they work for you. You believe the politicians are your representatives, and when they don&amp;rsquo;t represent your interests, you think they are dumb. No! They work for someone else. They use this democracy thing to only allow you to accept it. They are doing exactly what the people who pay them want them to do; in fact, they are doing it fantastically. Your confusion stems from thinking that they care about you. They never have.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;freedom&#34;&gt;Freedom&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to maintain illusion of freedom&lt;/strong&gt;: By putting freaks to the front. This person chopped his dick off, see we are free!(You are free to hurt yourself). But do not be fooled you are not free to say anything against the power, anything important.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;anxiety&#34;&gt;Anxiety&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You have anxiety because you should be anxious, Why is a mouse anxious ? Same for you! The large bear does not get anxious. A mouse think a noise (anxious) it might kill me! Bear thinks I might want to kill it! Its a different vibe.&lt;/li&gt;
&lt;li&gt;Anxiety is not a disorder - it is a natural conclusion of your ineptitude, its your soul giving you feedback. Anxiety is cured by strength.&lt;/li&gt;
&lt;li&gt;Dying ain&amp;rsquo;t so bad untill unless you die for a good reason. Dying is not the worst thing that can heppen, living without a purpose is the worst thing that can happen.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Asking Bugatti Owners How They Got RICH</title>
      <link>http://localhost:1313/posts/school-of-hard-knocks/asking-bugatti-owners-how-they-got-rich/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/school-of-hard-knocks/asking-bugatti-owners-how-they-got-rich/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/_sAyC_CLqzg?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=150&#34;&gt;2:30 - Fuck the Bussiness&lt;/a&gt;: It does not matter how large the business is, the only metric that matters is the money in your pocket. Even after the taxes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=193&#34;&gt;3:13 - Create friendships with your clients&lt;/a&gt;: And then be like brotheers to them.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=217&#34;&gt;3:37 - Most proffessors are too scared to enter the real world&lt;/a&gt;: That is why they keep professing! They are pros at nothing. They do not know how the real world works.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=233&#34;&gt;3:53 - Never quit&lt;/a&gt;: That is what separates winners from losers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=348&#34;&gt;5:48 - Watches YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=397&#34;&gt;6:41 - The world will teach you if you listen&lt;/a&gt;: Fail fast, fail cheap, fail often. If you never swing you never hit a homerun(thats the game right there).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=432&#34;&gt;7:12 - You have to love what you do, otherwise you do not work so hard&lt;/a&gt;: Do not make descisons that have potential to take joy away from the project.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=434&#34;&gt;7:14 - Always over deliver on the promise&lt;/a&gt;: Always keep your word. Even though there are contracts but if i shake your hand its a done deal..&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=617&#34;&gt;10:17 - Go through the day looking at the problems that can be solved&lt;/a&gt;: What&amp;rsquo;s a better way? Look for entreprenurial opportunities everywhere. And think out of the box.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=891&#34;&gt;14:51 - If you pay peanuts, you will attract monkeys&lt;/a&gt;: If you pay low, you will get guaranteed low quality.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=1034&#34;&gt;17:14 - Stop trying to convince people about your vision&lt;/a&gt;: Do not discuss your vision with all tom dick and harry.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/_sAyC_CLqzg?t=1127&#34;&gt;18:47 - People better than you will give permission to become better yourself&lt;/a&gt;: There can even be people in your family and friends who are not doing that, they are telling you to be humble, quit, grateful.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>http://localhost:1313/posts/prnn/26---backpropogation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/26---backpropogation/</guid>
      <description>&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$L$: number of layers in the network.&lt;/li&gt;
&lt;li&gt;$w_{jk}^l$: weight connecting $k^{th}$ neuron of $(l-1)^{th}$ layer to $j^{th}$ neuron of $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$b_j^l$: bias of $j^{th}$ neuron in $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$a_j^l$: output of $j^{th}$ neuron in $l^{th}$ layer&lt;/li&gt;
&lt;li&gt;$z_j^l$: preactivation output of $j^{th}$ neuron in $l^{th}$ layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here:
$$z_j^l = \sum_k w_{jk}^l a_k^{l-1} + b_j^l$$&lt;/p&gt;
&lt;p&gt;Also:
$$a_j^l = \sigma(z_j^l)$$
$$a_j^l = \sigma(\sum_k w_{jk}^l a_k^{l-1} + b_j^l)$$&lt;/p&gt;
&lt;h2 id=&#34;derivation&#34;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume we use squared loss function:
$$L = \frac{1}{2} |a^L - y|^2$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bagging and Random Forest</title>
      <link>http://localhost:1313/posts/prnn/30---bagging-and-random-forest/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/30---bagging-and-random-forest/</guid>
      <description>&lt;h2 id=&#34;bagging&#34;&gt;Bagging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let the original data be D.&lt;/li&gt;
&lt;li&gt;Create n new datasets by sampling with replacement from D.&lt;/li&gt;
&lt;li&gt;Each new dataset will have the same number of samples as the original dataset, but some samples will be repeated, and some will be excluded.&lt;/li&gt;
&lt;li&gt;Train a model on each of the new datasets.&lt;/li&gt;
&lt;li&gt;The final prediction is made by aggregating the predictions of all models:
&lt;ul&gt;
&lt;li&gt;For classification, the final prediction is made by taking the majority vote of the predictions of all models.&lt;/li&gt;
&lt;li&gt;For regression, the final prediction is made by taking the average of the predictions of all models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;random-forest---bagged-descion-trees&#34;&gt;Random Forest - Bagged descion trees&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bagging of decision trees.&lt;/li&gt;
&lt;li&gt;Each decision tree is trained on a random subset of the features.&lt;/li&gt;
&lt;li&gt;The final prediction is made by taking the majority vote of the predictions of all decision trees.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Bayes classifier</title>
      <link>http://localhost:1313/posts/prnn/5---bayes-classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/5---bayes-classifier/</guid>
      <description>&lt;p&gt;Let the data be D = {($x_1$, $y_1$), ($x_2$, $y_2$), &amp;hellip;, ($x_n$, $y_n$)}
where $y_i$ is the label of the feature vector $x_i$.&lt;/p&gt;
&lt;p&gt;Let the hypothesis space be $H = {h | h: X \to Y}$.&lt;/p&gt;
&lt;p&gt;The Bayes classifier is defined as:
$$h_B(x) = \begin{cases}
1 &amp;amp; \text{if } P(y=1 | x=x_i) &amp;gt; P(y=0 | x=x_i) \
0 &amp;amp; \text{if } P(y=1 | x=x_i) \leq P(y=0 | x=x_i)
\end{cases}$$&lt;/p&gt;
&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$P(y)$ is also called the prior probability of class $y$ or class prior probability.&lt;/li&gt;
&lt;li&gt;$P(x|y)$ is also called the likelihood of $x$ given class $y$.&lt;/li&gt;
&lt;li&gt;$P(y|x)$ is also called the posterior probability of class $y$ given $x$.&lt;/li&gt;
&lt;li&gt;$P(x)$ is also called the evidence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bayes-classifier-is-the-best-classifier-for-0-1-loss-function&#34;&gt;Bayes classifier is the best classifier for 0-1 loss function&lt;/h2&gt;
&lt;p&gt;To prove that the Bayes classifier is the best classifier for the 0-1 loss function, we need to show that it minimizes the expected risk (error) for any given input $x$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bias variance tradeoff</title>
      <link>http://localhost:1313/posts/prnn/19--bias-variance-tradeoff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/19--bias-variance-tradeoff/</guid>
      <description>&lt;p&gt;It can be shown that average risk for square error loss can be decomposed into three components:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$
R_{\text{avg}}(h) = \mathbb{E}_{P_D} \mathbb{E}_{P_{x,y}} \left[ \left( h_D(x) - \hat{h}(x) \right)^2 \right] \quad \text{// variance (sensitivity to dataset)}
$$
&lt;/div&gt;
$$
+ \mathbb{E}_{P_{x,y}} \left[ \left( \hat{h}(x) - h^*(x) \right)^2 \right] \quad \text{// bias (how different is avg classifier from optimum classifier)}
$$
&lt;div class=&#34;math-katex&#34;&gt;
$$
+ \mathbb{E}_{P_{x,y}} \left[ \left( h^*(x) - y \right)^2 \right] \quad \text{// irreducible noise (nothing can be done about this)}
$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Boosting</title>
      <link>http://localhost:1313/posts/prnn/31---boosting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/31---boosting/</guid>
      <description>&lt;p&gt;In bagging, we train each model independently on a random subset of the data. In boosting, we train each model sequentially on the same data, with each subsequent model focusing on correcting the errors of combined previous model by increasing weights of misclassified datapoints&lt;/p&gt;
&lt;p&gt;Unlike bagging, each model depends on the previous ones and its contribution to the final prediction is weighted differently.&lt;/p&gt;
&lt;h2 id=&#34;mathematical-formulation-of-boosting&#34;&gt;Mathematical Formulation of Boosting&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s define the ensemble model $H_T(x)$ as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cheat sheet</title>
      <link>http://localhost:1313/posts/adrl/cheatsheet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/cheatsheet/</guid>
      <description>&lt;h2 id=&#34;formulas&#34;&gt;Formulas:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;K L divergence&lt;/strong&gt;: $D_{KL}(p; q) = \int p(x) \log \frac{p(x)}{q(x)} dx$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;F-divergence&lt;/strong&gt;: $D_{F}(p \parallel q) = \int q(x) f\left(\frac{p(x)}{q(x)}\right) dx$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f: \mathbb{R}^+ \rightarrow \mathbb{R}$ is lower semi continous convex function such that $f(1) = 0$&lt;/li&gt;
&lt;li&gt;If $f(x) = x \log x$, then $D_{F}(p \parallel q) = D_{KL}(p \parallel q)$&lt;/li&gt;
&lt;li&gt;If $f(x) = x\log(x) - (x+1)\log(x+1)$, then $f^*=-\log(1-\exp^x)$, then $D_{F}(p \parallel q)$ = Jensen-Shannon divergence = $D_{JS}(p \parallel q) = \frac{1}{2}D_{KL}(p \parallel m) + \frac{1}{2}D_{KL}(q \parallel m)$, where $m = \frac{1}{2}(p + q)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jensen&amp;rsquo;s inequality&lt;/strong&gt;: For a convex function $f$ and a random variable $X$: $f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Class 2</title>
      <link>http://localhost:1313/posts/reinforcement-learning/10-jan-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/10-jan-2025/</guid>
      <description></description>
    </item>
    <item>
      <title>Conditional GANs(cGANs)</title>
      <link>http://localhost:1313/posts/adrl/10---conditional-gans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/10---conditional-gans/</guid>
      <description>&lt;p&gt;A Conditional Generative Adversarial Network (cGAN) is a type of Generative Adversarial Network (GAN) where the generation process is conditioned on some additional information, such as class labels or data from other modalities.&lt;/p&gt;
&lt;h2 id=&#34;architecture-modification&#34;&gt;Architecture modification&lt;/h2&gt;
&lt;h3 id=&#34;generator&#34;&gt;Generator&lt;/h3&gt;
&lt;p&gt;Concatenate the conditional information Y with the noise input Z and feed it to the generator.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/13.JPG&#34; alt=&#34;Image Description&#34; width=&#34;400&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;
&lt;p&gt;Here Y can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One hot encoded class label&lt;/li&gt;
&lt;li&gt;Text embedding etc&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discriminator&#34;&gt;Discriminator&lt;/h3&gt;
&lt;p&gt;The discriminator in a cGAN is modified similarly to the generator. Concatenate the conditional information Y with the input image X and feed this combined input (X, Y) to the discriminator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Conditional score matching</title>
      <link>http://localhost:1313/posts/adrl/24---conditional-score-matching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/24---conditional-score-matching/</guid>
      <description>&lt;p&gt;When we had unlabeled data, we estimated the score function:&lt;/p&gt;
&lt;p&gt;$$s(x) = \nabla_x \log p(x)$$&lt;/p&gt;
&lt;p&gt;Now, we have labeled data ie $D = {(x_i, y_i)}_{i=1}^N$ we instead estimate the conditional score function:&lt;/p&gt;
&lt;p&gt;$$s(x|y) = \nabla_x \log p(x|y)$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s(x)$ is the unconditional score function&lt;/li&gt;
&lt;li&gt;$s(x|y)$ is the conditional score function&lt;/li&gt;
&lt;li&gt;$p(x)$ is the data distribution&lt;/li&gt;
&lt;li&gt;$p(x|y)$ is the conditional data distribution given some condition $y$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This can be done in 2 ways:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convex conjugate</title>
      <link>http://localhost:1313/posts/adrl/4---convex-conjugate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/4---convex-conjugate/</guid>
      <description>&lt;p&gt;Convex conjugate is also known as the Fenchel conjugate or the Legendre transform&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Let $f: R^n → R$ be a convex function. The convex conjugate of $f$, denoted as $f^*$, is defined as:&lt;/p&gt;
&lt;p&gt;$$f^*(y) = \underset{x \in \text{dom} f}{\sup} (y^T x - f(x))$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$y \in R^n$ is the variable&lt;/li&gt;
&lt;li&gt;$\text{dom} f$ is the domain of $f$&lt;/li&gt;
&lt;li&gt;$\sup$ denotes the supremum (least upper bound)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;properties&#34;&gt;Properties&lt;/h2&gt;
&lt;h3 id=&#34;1-convexity-fx-is-also-convex&#34;&gt;1. &lt;strong&gt;Convexity&lt;/strong&gt;: $f^{**}(x)$ is also convex&lt;/h3&gt;
&lt;p&gt;Proof: Let $y_1, y_2 \in \mathbb{R}^n$ and $\lambda \in [0,1]$. We need to show that:&lt;/p&gt;</description>
    </item>
    <item>
      <title>DDIM (Denoising Diffusion Implicit Models)</title>
      <link>http://localhost:1313/posts/adrl/30---ddim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/30---ddim/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;During inference, we often had to sequentially traverse through 1000 time steps because of the Markov property assumption, which can be computationally intensive.&lt;/p&gt;
&lt;p&gt;Is it possible to give up the Markov property assumption so that we can train the same way but sample faster - not have to traverse through 1000 time steps?&lt;/p&gt;
&lt;h2 id=&#34;recall-the-ddpm&#34;&gt;Recall the DDPM&lt;/h2&gt;
&lt;p&gt;Forward process:
$$q(x_t | x_0) = \int q(x_{t:1} | x_0) dx_{1:(t-1)}$$
$$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_0, (1 - \alpha_t) \mathbb{I})$$
$$\Rightarrow x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon, \quad \epsilon \sim \mathcal{N}(.)$$
Note that here $\alpha_t$ is $\bar{\alpha}$ that we studied in DDPM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DDIM inversion</title>
      <link>http://localhost:1313/posts/adrl/31---ddim-inversion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/31---ddim-inversion/</guid>
      <description>&lt;h2 id=&#34;x_t-to-x_0&#34;&gt;$X_T$ to $X_0$&lt;/h2&gt;
&lt;p&gt;For $\sigma = 0$, the DDIM process becomes deterministic:&lt;/p&gt;
&lt;p&gt;$$x_{t-1} = \sqrt{\alpha_{t-1}}\left(\frac{x_t - \sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}}\epsilon_\theta^{(t)}(x_t)$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\epsilon_\theta^{(t)}(x_t)$ is the predicted noise from our trained model&lt;/li&gt;
&lt;li&gt;$\alpha_t$ is the cumulative product of $(1-\beta_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This deterministic nature means that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For a given $x_T$, we can find a unique $x_0$ by following the forward process&lt;/li&gt;
&lt;li&gt;Given this $x_T$, we can recover the exact same $x_0$ through the reverse process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This property makes DDIM particularly useful for:&lt;/p&gt;</description>
    </item>
    <item>
      <title>DDPMs as SDEs</title>
      <link>http://localhost:1313/posts/adrl/27-ddpm-as-sdes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/27-ddpm-as-sdes/</guid>
      <description>&lt;h2 id=&#34;step-1-of-big-picture--stochastic-forward-process-to-continuous-sde&#34;&gt;Step 1 of big picture  (Stochastic forward process to continuous SDE)&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s see how DDPMs can be viewed as discretizations of SDEs.&lt;/p&gt;
&lt;p&gt;The forward process in DDPMs is given by:
$$x_{i+1} = \sqrt{1-\beta_{i+1}}x_i + \sqrt{\beta_{i+1}}\epsilon_i \quad \text{where} \quad \epsilon_i \sim \mathcal{N}(0,I)$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s derive the corresponding continuous-time SDE:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, rewrite the discrete equation:
$$x_{i+1} - x_i = (\sqrt{1-\beta_{i+1}} - 1)x_i + \sqrt{\beta_{i+1}}\epsilon_i$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As the time step $\Delta t \to 0$, we can approximate:
$$\sqrt{1-\beta_{i+1}} \approx 1 - \frac{\beta_{i+1}}{2}$$
This approximation comes from the Taylor series expansion of $\sqrt{1-x}$ around $x=0$:
$$\sqrt{1-x} = 1 - \frac{1}{2}x - \frac{1}{8}x^2 - \frac{1}{16}x^3 + \cdots$$
We keep only the first two terms, assuming $\beta_{i+1}$ is small.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decision Trees</title>
      <link>http://localhost:1313/posts/prnn/28---descion-trees/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/28---descion-trees/</guid>
      <description>&lt;h2 id=&#34;how-to-descion-tree-works&#34;&gt;How to descion tree works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;At each node, a question is asked about the data that splits the data into two or more non-overlapping subsets.
$$
\text{Question: } x_j \leq \theta
$$
then the data is split into two subsets, one where $x_j \leq \theta$ and one where $x_j &amp;gt; \theta$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The process is repeated till we reach leaf node, that classifies the datapoint to a region of the feature space.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Models (DDPM) - part 1</title>
      <link>http://localhost:1313/posts/adrl/16---denoising-diffusion-probabilistic-models-ddpm---part-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/16---denoising-diffusion-probabilistic-models-ddpm---part-1/</guid>
      <description>&lt;h2 id=&#34;ddpm-as-special-case-of-variational-autoencoders-vae&#34;&gt;DDPM as special case of variational autoencoders (VAE)&lt;/h2&gt;
&lt;p&gt;Impose following structure on the VAE:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple latent variables, where each latent variable is associated with a timestep.&lt;/li&gt;
&lt;li&gt;Dim of all the latent variables is the same as the dim of the data.&lt;/li&gt;
&lt;li&gt;Assume a non learnable encoder unlike VAE where the encoder was learnable.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notations&#34;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$X_1, &amp;hellip;, X_T$: Denote latent variables at different timesteps&lt;/li&gt;
&lt;li&gt;$p(X_0)$: Denote the model data distribution&lt;/li&gt;
&lt;li&gt;$X_{1:T} = (X_1, X_2, &amp;hellip;, X_T)$: Denote the sequence of all latent variables&lt;/li&gt;
&lt;li&gt;$q(X_{1:T}|X_0)$: Denote the latent posterior&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;latent-posterior&#34;&gt;Latent posterior&lt;/h2&gt;
&lt;p&gt;Because $q(X_{1:T}|X_0)$ is not learnable, we need to define it - we make it first order markovian chain with gaussian transitions:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Models (DDPM) - part 2</title>
      <link>http://localhost:1313/posts/adrl/17---denoising-diffusion-probabilistic-models-ddpm---part-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/17---denoising-diffusion-probabilistic-models-ddpm---part-2/</guid>
      <description>&lt;p&gt;We were using latent variables $X_t$ to model the data as:&lt;/p&gt;
&lt;p&gt;$$\log p_\theta(x_0) = \log \int p_\theta(x_{0:T}) dx_{1:T}$$&lt;/p&gt;
&lt;p&gt;This formulation expresses the log-likelihood of the observed data $x_0$ as an integral over all possible latent variable sequences $x_{1:T}$. The model&amp;rsquo;s goal is to maximize this log-likelihood, which involves learning the parameters $\theta$ that define the generative process from $x_T$ back to $x_0$.&lt;/p&gt;
&lt;p&gt;Also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_\theta(x_T) = \mathcal{N}(0, I)$ - The prior distribution of the final latent variable is a standard normal distribution&lt;/li&gt;
&lt;li&gt;$p_\theta(x_{0:T}) = p_\theta(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t)$ - The joint distribution of all latent variables is factorized using the chain rule&lt;/li&gt;
&lt;li&gt;$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I)$ - The forward process transition probability is a Gaussian distribution&lt;/li&gt;
&lt;li&gt;$q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})$ - The posterior of all latent variables given the data is factorized using the chain rule&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because the log likelihood is intractable, we constructed the ELBO of the model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Models (DDPM) - part 3</title>
      <link>http://localhost:1313/posts/adrl/18---denoising-diffusion-probabilistic-modelsddpm---part-3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/18---denoising-diffusion-probabilistic-modelsddpm---part-3/</guid>
      <description>&lt;p&gt;Recall that:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$T_3 = - \sum_{t=2}^T \mathbb{E}_{q(x_t|x_0)} [D_{KL}(q(x_{t-1}|x_t,x_0) \| p_\theta(x_{t-1}|x_t))]$$
&lt;/div&gt;
Here:
&lt;ul&gt;
&lt;li&gt;$q(x_{t-1}|x_t,x_0)$ is the noising distribution after applying the forward process for $t-1$ steps&lt;/li&gt;
&lt;li&gt;$p_\theta(x_{t-1}|x_t)$ is the model&amp;rsquo;s predicted denoising distribution after applying the reverse process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also know that:&lt;/p&gt;
&lt;p&gt;$$q(x_{t-1}|x_t,x_0) = \mathcal{N}(x_{t-1}; \mu_q(x_t,x_0), \Sigma_q(t))$$&lt;/p&gt;
&lt;p&gt;where $\mu_q(x_t,x_0)$ and $\Sigma_q(t)$ are the mean and covariance of the posterior distribution and:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$
\mu_q(x_t, x_0) = \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\bar{\alpha}_t}
$$
&lt;/div&gt;
&lt;p&gt;The covariance $\Sigma_q(t)$ is given by:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Difference between SMLD and DDPM</title>
      <link>http://localhost:1313/posts/adrl/23---difference-between-smld-and-ddpm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/23---difference-between-smld-and-ddpm/</guid>
      <description>&lt;p&gt;Note that: NCSN: Noise Conditional Score Network is same as SMLD: Score matching Langevin Dynamics&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;The inference processes also differ between SMLD and DDPM:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;SMLD (Noise Conditional Score Networks):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We start with a sequence of noise scales arranged from largest to smallest:
$${\sigma_i}_{i=1}^L, \quad \sigma_1 &amp;gt; \sigma_2 &amp;gt; &amp;hellip; &amp;gt; \sigma_L \quad (L \approx 10)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start with random Gaussian noise:
$$x_0 \sim \mathcal{N}(0, \sigma_1^2I)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each noise scale $\sigma_i$ from $i=1$ to $L$:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distillation</title>
      <link>http://localhost:1313/posts/adrl/37---distillation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/37---distillation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Teacher model is a large model that was trained on a large dataset as usual.&lt;/li&gt;
&lt;li&gt;Student model is a smaller model that is trained to mimic the behavior of the teacher model.&lt;/li&gt;
&lt;li&gt;The student model has 2 losses:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$
\mathcal{L} = \mathcal{L}_\text{task} + \mathcal{L}_\text{distill}
$$
&lt;/div&gt;
  - $\mathcal{L}_\text{task}$ is the task-specific loss.
  - $\mathcal{L}_\text{distill}$ is the distillation loss - This loss is used to make the student model&#39;s output close to the teacher model&#39;s softened outputs.</description>
    </item>
    <item>
      <title>Domain adversarial networks (DANs)</title>
      <link>http://localhost:1313/posts/adrl/11---domain-adversarial-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/11---domain-adversarial-networks/</guid>
      <description>&lt;p&gt;Also known as Unsupervised domain adaptation (UDA)&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Domain adversarial networks (DANs) are a type of neural network architecture designed to address the problem of domain shift, where the distribution of data differs between the training and testing phases. DANs use adversarial training to learn domain-invariant features that are robust across different domains.&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;p&gt;The aim is to learn a classifier that performs well on both the source and target domains, even though the target domain may have different characteristics than the source domain. For example, we train a model to classify medical images at company A and then we want to use this model to classify medical images at some other hospital.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ensemble learning</title>
      <link>http://localhost:1313/posts/prnn/29---ensemble-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/29---ensemble-learning/</guid>
      <description>&lt;p&gt;Ensemble learning is a machine learning technique that combines multiple models to improve the overall performance and robustness of the prediction. It is based on the idea that by aggregating the predictions of several models, we can achieve better results than any single model alone.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Expectation Maximization (EM) algorithm</title>
      <link>http://localhost:1313/posts/prnn/8---em-algorithm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/8---em-algorithm/</guid>
      <description>&lt;p&gt;Let the data be $D = {t_i}_{i=1}^N$ where $t_i \in \mathbb{R}^d$ are iid $p_t(t)$ data points.&lt;/p&gt;
&lt;h2 id=&#34;latent-variable-models&#34;&gt;Latent variable models&lt;/h2&gt;
&lt;p&gt;Let each data point $x_i$ be associated with a latent variable $z_i$ that is not observed. $z_i$ is a random variable that takes values in some finite set ${1, \ldots, K}$ and represents the membership of $x_i$ to one of $K$ clusters.&lt;/p&gt;
&lt;p&gt;Hence the data can be represented as&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$D = \{(t_i, z_i)\}_{i=1}^N$$
&lt;/div&gt;
 where $t_i$ is the observed data and $z_i$ is the latent variable and $(t_i,z_i)$ are iid $p_{tz}$.
&lt;p&gt;The marginal distribution of the observed data is given by:&lt;/p&gt;</description>
    </item>
    <item>
      <title>F-Divergence</title>
      <link>http://localhost:1313/posts/adrl/3---f-divergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/3---f-divergence/</guid>
      <description>&lt;p&gt;F-divergence is a generalized measure of difference between two probability distributions. For probability distributions $P$ and $Q$ over a event space $X$, the F-divergence is defined as:&lt;/p&gt;
&lt;p&gt;$$D_f(P || Q) = ∫_X q(x) f(\frac{p(x)}{q(x)}) dx$$
Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generator function&lt;/strong&gt;  $f: R^+ → R$ is a lower semi-continuous convex function with $f(1) = 0$.&lt;/li&gt;
&lt;li&gt;The F in F-divergence comes from generator &amp;ldquo;f&amp;quot;unction.&lt;/li&gt;
&lt;li&gt;$p(x)$ and $q(x)$ are the probability density functions of $P$ and $Q$ respectively&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some examples of F-divergences include:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Frechet Inception Distance (FID)</title>
      <link>http://localhost:1313/posts/adrl/9--frechet-inception-distance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/9--frechet-inception-distance/</guid>
      <description>&lt;h2 id=&#34;why-generative-models-cannot-be-evaluated-like-discriminative-models&#34;&gt;Why generative models cannot be evaluated like discriminative models&lt;/h2&gt;
&lt;p&gt;Usually, we train a deep learning model on training data and then evaluate it on test data. However, for generative models, we cannot directly compare the generated samples to the test data in the same way we evaluate discriminative models. This is because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generative models produce new, unique samples rather than predictions on existing data.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s no one-to-one correspondence between generated samples and real data points.&lt;/li&gt;
&lt;li&gt;We need to assess the quality, diversity, and realism of the generated samples, which requires different metrics.&lt;/li&gt;
&lt;li&gt;Traditional evaluation metrics like accuracy or mean squared error are not applicable to generative tasks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, we need specialized methods to evaluate generative models, such as the Frechet Inception Distance (FID), which measures the similarity between the distribution of generated samples and the distribution of real data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Free Resources</title>
      <link>http://localhost:1313/posts/free-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/free-resources/</guid>
      <description>&lt;h3 id=&#34;-read-my-free-books&#34;&gt;📖 Read my free books&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://tinyurl.com/mstwwp9p&#34;&gt;Machine Learning Book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tinyurl.com/mr48tzre&#34;&gt;Deep Learning Book&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-learn-ai-for-free&#34;&gt;🎯 Learn AI for Free&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://discord.gg/QHAbprqQme&#34;&gt;Complete AI Roadmap &amp;amp; Learning Community&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-keep-up-to-date-with-ai-progress&#34;&gt;🔄 Keep up to date with AI progress&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/victor-explore/AI-Content-Creators&#34;&gt;Directory of AI Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-read-my-handwritten-notes-for-free&#34;&gt;📚 Read my handwritten notes for free&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/6073454c36&#34;&gt;Linear Algebra (Basics)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/007f5338ee&#34;&gt;Linear Algebra (Advanced)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/142021448f&#34;&gt;Probability (Basics)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/6efea56383&#34;&gt;Probability (Advanced)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/293e2d1674&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/cde4448eb9&#34;&gt;Game Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/30162e31b7&#34;&gt;Data Structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/s/5f91510db0&#34;&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-iisc-bangalore-resources&#34;&gt;🎓 IISC Bangalore Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/victor-explore/AI-Assignments-IISC-Banglore&#34;&gt;AI Assignments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/victor-explore/AI-Q-Papers-IISC-Banglore&#34;&gt;AI Q Papers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-support-me&#34;&gt;💝 Support Me&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ko-fi.com/victor_explore&#34;&gt;Buy me a coffee&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Generalized Linear Regression</title>
      <link>http://localhost:1313/posts/prnn/12.1---generalized-linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/12.1---generalized-linear-regression/</guid>
      <description>&lt;p&gt;Generalized Linear Regression extends the concept of linear regression by projecting the data into a higher-dimensional space before performing linear regression. This allows us to capture more complex relationships in the data.&lt;/p&gt;
&lt;p&gt;Let the data be $(x_i, y_i)$ for $i = 1, 2, &amp;hellip;, n$ where $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$.&lt;/p&gt;
&lt;p&gt;We define a feature map $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^m$, where $m &amp;gt; d$. This map projects our original features into a higher-dimensional space.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generative Adversarial Networks (GANs)</title>
      <link>http://localhost:1313/posts/adrl/5---gans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/5---gans/</guid>
      <description>&lt;p&gt;Recall that generative models try to learn the data distribution $P_{data}$.&lt;/p&gt;
&lt;p&gt;GANs try to learn $P_{data}$ by approximating it by $P_{generator}$ by minimizing F-divergence $D_f(P_{data} || P_{generator})$.&lt;/p&gt;
&lt;h2 id=&#34;derivation-of-gans&#34;&gt;Derivation of GANs&lt;/h2&gt;
&lt;p&gt;Let $\theta$ be the parameters of the generator such that it maps a sample $z \sim N(0, I)$ to a sample $x \sim p_{generator}$.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/5.JPG&#34; alt=&#34;Image Description&#34; width=&#34;500&#34; height=&#34;auto&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;Then, the optimal parameters for the generator can be expressed as:&lt;/p&gt;
&lt;p&gt;$$\theta^* = \underset{\theta}{\text{argmin}} D_f(P_{data} || P_{generator})$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generative vs Discriminative Models</title>
      <link>http://localhost:1313/posts/adrl/1--what-is-the-difference-between-generative-model-and-discriminative-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/1--what-is-the-difference-between-generative-model-and-discriminative-model/</guid>
      <description>&lt;h2 id=&#34;generative-model&#34;&gt;Generative Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let the training dataset be&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$data = \{(x_i)\}_{i=1}^N$$
&lt;/div&gt;
&lt;p&gt;where $x_i \in \mathbb{R}^d$ are called data points. These data points are iid samples from the true data distribution $P(x)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Aim of generative model is to extimate $P(x)$ using the training data and sample new data points from it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GANs, GMMs etc are a example of generative model. In GANs we use training data to train a neural network to learn the distribution of the data. Once we have learnt this distribution we can sample new data points from it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How This 76 Year Old Turned thousands into millions</title>
      <link>http://localhost:1313/posts/school-of-hard-knocks/how-this-76-year-old-turned-1000-into-a-66m-empire/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/school-of-hard-knocks/how-this-76-year-old-turned-1000-into-a-66m-empire/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nswPe9PAgjo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=178&#34;&gt;2:58 - Bullshit flies if you are selling&lt;/a&gt;: And its OK. People will believe it.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=217&#34;&gt;3:37 - People will buy it&lt;/a&gt;: You just have to say it and people will buy it.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=243&#34;&gt;4:03 - Urgency is a great motivator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=293&#34;&gt;4:53 - Learn to listen, people are always talking&lt;/a&gt;: The harder you listen, the better you will be at making deal.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=461&#34;&gt;7:41 - Want it so bad that you can almost taste it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=611&#34;&gt;10:11 - Who is a great entrepreneur?&lt;/a&gt;: Someone who is going to get back up. This is the &amp;ldquo;only&amp;rdquo; difference. You are mostly born with it, but can be exagerrated.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=661&#34;&gt;11:01 - I alwayas loose money, who cares, its a long long game&lt;/a&gt;: Going up and down have fun.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=725&#34;&gt;12:05 - You will always have doubts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=915&#34;&gt;15:15 - Be authentic&lt;/a&gt;: Everyone like the authentic ones.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/nswPe9PAgjo?t=980&#34;&gt;16:20 - Nothing wrong with over preparing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Hypothesis space (H)</title>
      <link>http://localhost:1313/posts/prnn/1---hypothesis-space/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/1---hypothesis-space/</guid>
      <description>&lt;p&gt;Let the data be D = {($x_1$, $y_1$), ($x_2$, $y_2$), &amp;hellip;, ($x_n$, $y_n$)}
where $y_i$ is the label of the feature vector $x_i$.&lt;/p&gt;
&lt;p&gt;A hypothesis function $h$ is a function that maps the feature vector $x$ to the label $y$, ie
$$h: X \to Y$$&lt;/p&gt;
&lt;p&gt;The hypothesis space is the set of all possible hypothesis functions, denoted as
$$H = {h | h: X \to Y}$$&lt;/p&gt;
&lt;p&gt;During the learning process, we try to find the best hypothesis function $h$ from the hypothesis space $H$ that minimizes the error between the predicted label and the true label.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Index for Artificial Intelligence Topics</title>
      <link>http://localhost:1313/posts/prnn/0---index/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/0---index/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/1---hypothesis-space/&#34;&gt;Hypothesis Space&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/2---loss-function/&#34;&gt;Loss Functions&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/3---risk-function/&#34;&gt;Risk and Empirical Risk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/4---learning-problem/&#34;&gt;Learning Problem&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/6---kl-divergence---kullback-leibler-divergence/&#34;&gt;KL Divergence - Kullback-Leibler Divergence&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/7---likelyhood/&#34;&gt;Minimizing KL Divergence is Equivalent to Maximizing Likelihood&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/5---bayes-classifier/&#34;&gt;Bayes Classifier&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/13---logistic-regression/&#34;&gt;Logistic regression also known as logit regression or binary classification&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/14---softmax-regression/&#34;&gt;Softmax regression also known multiclass regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/8---em-algorithm/&#34;&gt;Expectation Maximization (EM) Algorithm&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/12---linear-regression/&#34;&gt;Linear Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/12.1---generalized-linear-regression/&#34;&gt;Generalized Linear Regression&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/16---non-parametric-density-estimation/&#34;&gt;Non-parametric Density Estimation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/17---parzen-window-estimate/&#34;&gt;Parzen Window Estimate&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/18---k-nearest-neighbour/&#34;&gt;K-Nearest Neighbour (KNN)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/9---linear-discriminant-analysis/&#34;&gt;Linear Discriminant Analysis (LDA)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/15---maximum-aposteriori-estimate/&#34;&gt;Maximum A Posteriori Estimate (MAP)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/prnn/19--bias-variance-tradeoff/&#34;&gt;Bias variance tradeoff&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Index for Deep Learning Topics</title>
      <link>http://localhost:1313/posts/adrl/0---index/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/0---index/</guid>
      <description>&lt;h2 id=&#34;gans&#34;&gt;GANs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/1--what-is-the-difference-between-generative-model-and-discriminative-model/&#34;&gt;What is the difference between generative model and discriminative models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/2---kl-divergence---kullback-leibler-divergence/&#34;&gt;KL Divergence - Kullback-Leibler Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/3---f-divergence/&#34;&gt;F-Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/4---convex-conjugate/&#34;&gt;Convex conjugate (also known as the Fenchel conjugate or the Legendre transform)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/5---gans/&#34;&gt;GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/6---naive-gans/&#34;&gt;Naive GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/7--wasserstein-metric/&#34;&gt;Wasserstein metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/8---wasserstein-gans-/&#34;&gt;Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/9--frechet-inception-distance/&#34;&gt;Frechet Inception Distance (FID)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/10---conditional-gans/&#34;&gt;Conditional GANs(cGANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/11---domain-adversarial-networks/&#34;&gt;Domain adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;vaes&#34;&gt;VAEs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/12---variational-encoding/&#34;&gt;Variational encoding(VAEs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/12.1---two-types-of-nns/&#34;&gt;Two types of neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/13---aggregated-posterior-mismatch-/&#34;&gt;Aggregated posterior mismatch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/14---vae-as-regularized-autoencoder/&#34;&gt;VAE as regularized autoencoder and Beta VAEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/14.1-info-vae/&#34;&gt;Info VAEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/15---vqvae/&#34;&gt;Vector Quantized VAEs (VQ-VAEs)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ddpm&#34;&gt;DDPM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/16---denoising-diffusion-probabilistic-models-ddpm---part-1/&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM) - part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/17---denoising-diffusion-probabilistic-models-ddpm---part-2/&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM) - part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/18---denoising-diffusion-probabilistic-modelsddpm---part-3/&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM) - part 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/19---summarizing-ddpm/&#34;&gt;Summarizing DDPM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;score-matching&#34;&gt;Score matching&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/20---score-matching-part-1/&#34;&gt;Score matching part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/21---score-matching-part-2/&#34;&gt;Score matching part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/22---score-matching-part-3/&#34;&gt;Score matching part 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/23---difference-between-smld-and-ddpm/&#34;&gt;Difference between SMLD and DDPM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/24---conditional-score-matching/&#34;&gt;Conditional score matching&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sdes&#34;&gt;SDEs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/25--precursor-to-stochastic-differential-equations-sdes/&#34;&gt;Precursor to Stochastic Differential Equations (SDEs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/26---stochastic-differential-equations-sdes/&#34;&gt;Stochastic Differential Equations (SDEs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/26.1---some-example-problems/&#34;&gt;Some example problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/26.2-andersons-result/&#34;&gt;Anderson&amp;rsquo;s result&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/27-ddpm-as-sdes/&#34;&gt;DDPMs as SDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/28---smld-as-sde/&#34;&gt;SMLD as SDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/29---main-takeaway/&#34;&gt;Main takeaway&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/30---ddim/&#34;&gt;DDIM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/31---ddim-inversion/&#34;&gt;DDIM inversion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sequence2sequence-models&#34;&gt;Sequence2sequence models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/transformers/31.5/&#34;&gt;Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/32-state-space-models/&#34;&gt;State space models &amp;amp; Mamba&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self supervised learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/33---noise-contrastive-estimation/&#34;&gt;Noise contrastive estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/34---info-nce/&#34;&gt;Info-NCE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/35---masked-reconstruction/&#34;&gt;Masked reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/36---jepa/&#34;&gt;JEPA - Joint Embedding Prediction and Autoencoding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/37---distillation/&#34;&gt;Distillation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cheatsheet&#34;&gt;Cheatsheet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/adrl/cheatsheet/&#34;&gt;Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Index for Reinforcement Learning</title>
      <link>http://localhost:1313/posts/reinforcement-learning/0-index/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/0-index/</guid>
      <description>&lt;p&gt;This is the index for the Reinforcement Learning series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/reinforcement-learning/1-introduction-to-reinforcement-learning/&#34;&gt;Core idea of Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/posts/reinforcement-learning/2-multi-armed-bandit-problem/&#34;&gt;Multi-Armed Bandit Problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Infinite horizon discounted cost problems</title>
      <link>http://localhost:1313/posts/reinforcement-learning/11-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/11-feb-2025/</guid>
      <description>&lt;p&gt;In infinite horizon discounted cost problems, we consider scenarios without a termination state and an infinite sequence of decisions. Let&amp;rsquo;s understand the key components and mathematical formulation.&lt;/p&gt;
&lt;h3 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;State Space&lt;/strong&gt;: Set of states $S = {1,2,&amp;hellip;,n}$. Note that there is no terminal state unlike in finite horizon problems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action Space&lt;/strong&gt;: For each state $i$, set of feasible actions $A(i)$
&lt;ul&gt;
&lt;li&gt;Total action space: $A = \bigcup_{i \in S} A(i)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cardinality&lt;/strong&gt;: Both $|S|$ and $|A|$ are finite&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimal-cost-function&#34;&gt;Optimal Cost Function&lt;/h3&gt;
&lt;p&gt;The optimal cost function $J^*(i)$ represents the minimum total discounted cost starting from state $i$:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Info VAEs: Information Maximizing Variational Autoencoders</title>
      <link>http://localhost:1313/posts/adrl/14.1-info-vae/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/14.1-info-vae/</guid>
      <description>&lt;p&gt;InfoVAE addresses the aggregated posterior mismatch problem in standard VAEs by modifying the objective function to explicitly match the aggregated posterior with the prior distribution.&lt;/p&gt;
&lt;h2 id=&#34;key-ideas&#34;&gt;Key Ideas&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;If the aggregated posterior $q_\phi(z)$ becomes a Dirac delta, all inputs $x$ map to the same latent code $z$&lt;/li&gt;
&lt;li&gt;In such a scenario, the latent code $z$ would contain no information about the input $x$&lt;/li&gt;
&lt;li&gt;The standard VAE objective does not directly encourage the matching between $q_\phi(z)$ and the prior $p(z)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;infovae-objective-function&#34;&gt;InfoVAE Objective Function&lt;/h2&gt;
&lt;p&gt;The standard VAE objective is given by:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Info-NCE</title>
      <link>http://localhost:1313/posts/adrl/34---info-nce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/34---info-nce/</guid>
      <description>&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Instead of using 1 negative sample, why not use more?&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Let us formalize the Info-NCE framework:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A data point $x \in \mathcal{X}$ from input space $\mathcal{X}$&lt;/li&gt;
&lt;li&gt;A context $c \in \mathcal{C}$ from context space $\mathcal{C}$&lt;/li&gt;
&lt;li&gt;A positive sampling distribution $p_{pos}(x,c)$&lt;/li&gt;
&lt;li&gt;A negative sampling distribution $p_{neg}(x,c)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We sample:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One positive sample $x^+ \sim p_{pos}(x,c)$&lt;/li&gt;
&lt;li&gt;$N-1$ negative samples&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-katex&#34;&gt;$\{x^-_i\}_{i=1}^{N-1} \sim p_{neg}(x,c)$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The InfoNCE loss is defined as:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;math-katex&#34;&gt;
   $$\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[\log \frac{\exp(f_\theta(x)^\top f_\theta(x^+))}{\exp(f_\theta(x)^\top f_\theta(x^+)) + \sum_{i=1}^{N-1} \exp(f_\theta(x)^\top f_\theta(x^-_i))}\right]$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Reinforcement Learning</title>
      <link>http://localhost:1313/posts/reinforcement-learning/1-introduction-to-reinforcement-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/1-introduction-to-reinforcement-learning/</guid>
      <description>&lt;h2 id=&#34;core-idea-of-reinforcement-learning&#34;&gt;Core Idea of Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is a computational approach to learning whereby an &lt;strong&gt;agent&lt;/strong&gt; seeks to maximize some notion of cumulative reward by taking actions in an &lt;strong&gt;environment&lt;/strong&gt;. This process is akin to training a dog to perform a trick: you cannot dictate every move, but you can provide rewards when it performs correctly. In RL, the agent learns through trial and error, receiving feedback in the form of &lt;strong&gt;rewards&lt;/strong&gt; or penalties based on its &lt;strong&gt;actions&lt;/strong&gt;. The primary objective is to develop a &lt;strong&gt;policy&lt;/strong&gt;, a strategy that guides the agent to take actions that maximize the total accumulated reward over time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>JEPA - Joint Embedding Prediction and Autoencoding</title>
      <link>http://localhost:1313/posts/adrl/36---jepa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/36---jepa/</guid>
      <description>&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Joint Embedding Predictive Architecture (JEPA) combines predictive learning with representation learning in a unified framework.&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Let us formalize the JEPA framework:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input space $\mathcal{X}$&lt;/li&gt;
&lt;li&gt;Context patch $x_c \in \mathcal{X}$&lt;/li&gt;
&lt;li&gt;Target patch $x_t \in \mathcal{X}$&lt;/li&gt;
&lt;li&gt;Context encoder $f_\theta: \mathcal{X} \rightarrow \mathcal{Z}$&lt;/li&gt;
&lt;li&gt;Target encoder $g_\phi: \mathcal{X} \rightarrow \mathcal{Z}$&lt;/li&gt;
&lt;li&gt;Predictor network $h_\psi: \mathcal{Z} \rightarrow \mathcal{Z}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The process works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encode context: $z_c = f_\theta(x_c)$&lt;/li&gt;
&lt;li&gt;Encode target: $z_t = g_\phi(x_t)$&lt;/li&gt;
&lt;li&gt;Predict target embedding:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-katex&#34;&gt;$\hat{z}_t = h_\psi(z_c)$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The objective is to minimize prediction error in embedding space:&lt;/p&gt;</description>
    </item>
    <item>
      <title>K Nearest neighbour (KNN)</title>
      <link>http://localhost:1313/posts/prnn/18---k-nearest-neighbour/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/18---k-nearest-neighbour/</guid>
      <description>&lt;h2 id=&#34;basic-idea&#34;&gt;Basic idea&lt;/h2&gt;
&lt;p&gt;Recall that we had derived the following equation for non-parametric density estimation:&lt;/p&gt;
&lt;p&gt;$$p(x) = \frac{k}{nV}$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$k$ is the number of data points in region $R$&lt;/li&gt;
&lt;li&gt;$n$ is the total number of data points in the dataset $D$&lt;/li&gt;
&lt;li&gt;$V$ is the volume of the region $R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In K-Nearest Neighbour (KNN) method, we fix the volume $V$ and count $k$&lt;/p&gt;
&lt;h2 id=&#34;problem-setting&#34;&gt;Problem setting&lt;/h2&gt;
&lt;p&gt;Given a set of data points $D = {x_1, y_1}, {x_2, y_2}, &amp;hellip;, {x_n, y_n}$, where $x_i \in \mathbb{R}^d$ is the feature vector and $y_i \in {1, 2, &amp;hellip;, C}$ is the class label.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K-means clustering</title>
      <link>http://localhost:1313/posts/prnn/35---k-means-clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/35---k-means-clustering/</guid>
      <description>&lt;p&gt;K-means clustering also known as K-means algorithm or hard clustering&lt;/p&gt;
&lt;h2 id=&#34;recall-that-in-mixture-model-clustering&#34;&gt;Recall that in mixture model clustering&lt;/h2&gt;
&lt;p&gt;Data was:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$D = \{x_i\}_{i=1}^N$$
&lt;/div&gt;
 We assumed latent variable $z_i$ is associated with each data point $x_i$. Hence data became $D = \{(x_i, z_i)\}_{i=1}^N$.
&lt;p&gt;We assumed&lt;/p&gt;
&lt;p&gt;$$p_\theta(x) = \sum_{j=1}^m p_\theta(x|z=j) p_\theta(z=j) = \sum_{j=1}^m N(x; \mu_j, \Sigma_j) \pi_j$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N(x; \mu_j, \Sigma_j)$ is the Gaussian probability density function for cluster $j$, with mean $\mu_j$ and covariance matrix $\Sigma_j$.&lt;/li&gt;
&lt;li&gt;$\pi_j$ is the probability of a data point belonging to cluster $j$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-means clustering&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s make the following assumptions to simplify our model:&lt;/p&gt;</description>
    </item>
    <item>
      <title>KL Divergence - Kullback-Leibler Divergence</title>
      <link>http://localhost:1313/posts/adrl/2---kl-divergence---kullback-leibler-divergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/2---kl-divergence---kullback-leibler-divergence/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;KL divergence is a measure of how one probability distribution differs from another probability distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mathematically, for two probability distributions $P(x)$ and $Q(x)$, the KL divergence from $Q$ to $P$ is defined as:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$D_{KL}(P || Q) = \sum_{x} p(x) \log\left(\frac{p(x)}{q(x)}\right) = E_{x \sim p(x)}\left[\log\left(\frac{p(x)}{q(x)}\right)\right]$$ for discrete distributions&lt;/p&gt;
&lt;p&gt;$$D_{KL}(P || Q) = \int p(x) \log\left(\frac{p(x)}{q(x)}\right) dx = E_{x \sim p(x)}\left[\log\left(\frac{p(x)}{q(x)}\right)\right]$$ for continuous distributions&lt;/p&gt;
&lt;p&gt;where the sum/integral is over all possible events $x$. And $p(x)$ and $q(x)$ are the probability density functions of distributions $P(x)$ and $Q(x)$ respectively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>KL Divergence - Kullback-Leibler Divergence</title>
      <link>http://localhost:1313/posts/prnn/6---kl-divergence---kullback-leibler-divergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/6---kl-divergence---kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;KL divergence is a measure of how one probability distribution diverges from second, probability distribution.&lt;/p&gt;
&lt;p&gt;Mathematically, for two probability distributions $P(x)$ and $Q(x)$, the KL divergence from $Q$ to $P$ is defined as:&lt;/p&gt;
&lt;p&gt;$D_{KL}(p || q) = Σ p(x) * log\left(\frac{p(x)}{q(x)}\right)$ for discrete distributions&lt;/p&gt;
&lt;p&gt;$D_{KL}(p || q) = ∫ p(x) * log\left(\frac{p(x)}{q(x)}\right) dx$ for continuous distributions&lt;/p&gt;
&lt;p&gt;where the sum/integral is over all possible events $x$. And $p(x)$ and $q(x)$ are the probability density functions of distributions $P(x)$ and $Q(x)$ respectively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning problem</title>
      <link>http://localhost:1313/posts/prnn/4---learning-problem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/4---learning-problem/</guid>
      <description>&lt;p&gt;We are given a dataset $D = {(x_i, y_i)}_{i=1}^n$ sampled from an unknown distribution $P$.&lt;/p&gt;
&lt;p&gt;We want to find a hypothesis function $h$ out of the complete hypothesis space $H$ that minimizes the risk function $R(h)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning the system model from data and Monte Carlo methods</title>
      <link>http://localhost:1313/posts/reinforcement-learning/19-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/19-feb-2025/</guid>
      <description>&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;p&gt;We have covered several key topics in reinforcement learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fundamentals and Problem Formulation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basic concepts and terminology of reinforcement learning and problem formulation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-armed bandit problems(Single stage multiple action problems)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solved using greedy, ε-greedy, UCB, and gradient-based methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Markov Decision Processes ie MDPs(Known system model (state transition probabilities and reward/cost functions))&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Infinite horizon problems($n&amp;lt;\infty$ but deterministic) - Dynamic programming solutions to the Bellman equation for finding the optimal policy&lt;/li&gt;
&lt;li&gt;Stochastic shortest path problems($n&amp;lt;\infty$ but random) - Bellman equation formulations, value iteration, policy iteration&lt;/li&gt;
&lt;li&gt;Discounted cost problems ($n \to \infty$) - Bellman equation formulations, policy iteration, value iteration&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-the-system-model-from-data&#34;&gt;Learning the system model from data&lt;/h2&gt;
&lt;p&gt;From now on we shall assume we do not know the system model (state transition probabilities and reward/cost functions) and we need to learn it from data.
The Data is in the form of trajectories generated by the system:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Discriminant Analysis(LDA)</title>
      <link>http://localhost:1313/posts/prnn/9---linear-discriminant-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/9---linear-discriminant-analysis/</guid>
      <description>&lt;p&gt;Let the data be $D = {(x_i, y_i)}_{i=1}^N$ where $x_i \in \mathbb{R}^d$ and $y_i \in {1, 2}$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume the parametric form of the conditional density $p(x|y)$ is:&lt;/p&gt;
&lt;p&gt;$$p(x|y=1) \sim N(x; \mu_1, \Sigma)$$
$$p(x|y=0) \sim N(x; \mu_2, \Sigma)$$&lt;/p&gt;
&lt;p&gt;where $N(x; \mu, \Sigma)$ denotes a multivariate Gaussian distribution with mean $\mu$ and covariance matrix $\Sigma$. Note that we assume the covariance matrix $\Sigma$ is the same for both classes, which is a key assumption in LDA.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear regression</title>
      <link>http://localhost:1313/posts/prnn/12---linear-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/12---linear-regression/</guid>
      <description>&lt;p&gt;Let the data be $(x_i, y_i)$ for $i = 1, 2, &amp;hellip;, n$ where $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$.&lt;/p&gt;
&lt;p&gt;then we can model $y_i$ as:&lt;/p&gt;
&lt;p&gt;$$Y = \beta_0 + \beta_1X + \varepsilon$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Y \in \mathbb{R}$ is the dependent variable&lt;/li&gt;
&lt;li&gt;$X \in \mathbb{R}^d$ is the independent variable (or feature vector in higher dimensions)&lt;/li&gt;
&lt;li&gt;$\beta_0$ is the y-intercept (bias term)&lt;/li&gt;
&lt;li&gt;$\beta_1$ is the slope (or coefficient vector in higher dimensions)&lt;/li&gt;
&lt;li&gt;$\varepsilon$ is the error term&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ideal-regressor&#34;&gt;Ideal regressor&lt;/h2&gt;
&lt;p&gt;For mean squared error loss, the ideal regressor is defined as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic regression also known as logit regression or binary classification</title>
      <link>http://localhost:1313/posts/prnn/13---logistic-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/13---logistic-regression/</guid>
      <description>&lt;p&gt;Let the data be D = {($x_1$, $y_1$), ($x_2$, $y_2$), &amp;hellip;, ($x_n$, $y_n$)}
where $y_i \in {0, 1}$ is the binary label of the feature vector $x_i$.&lt;/p&gt;
&lt;p&gt;Then the logistic regression model is defined as:
$$P(y_i = 1 | x_i) = \frac{1}{1 + e^{-w^T x_i+b}}$$
$$P(y_i = 0 | x_i) = \frac{1}{1 + e^{w^T x_i+b}}$$&lt;/p&gt;
&lt;p&gt;where $w$ is the parameter vector.&lt;/p&gt;
&lt;p&gt;Note that
$$P(y_i = 1 | x_i) + P(y_i = 0 | x_i) = 1$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss function (L)</title>
      <link>http://localhost:1313/posts/prnn/2---loss-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/2---loss-function/</guid>
      <description>&lt;p&gt;Let the data be D = {($x_1$, $y_1$), ($x_2$, $y_2$), &amp;hellip;, ($x_n$, $y_n$)}
where $y_i$ is the label of the feature vector $x_i$.&lt;/p&gt;
&lt;p&gt;Let the hypothesis function be $h: X \to Y$.&lt;/p&gt;
&lt;p&gt;Let $\hat{y_i} = h(x_i)$ be the prediction of this hypothesis function for the feature vector $x_i$, whereas the true label is $y_i$.&lt;/p&gt;
&lt;p&gt;Then the loss function $L(y_i, \hat{y_i})$ is a function that measures the error between the predicted label and the true label.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Main takeaway</title>
      <link>http://localhost:1313/posts/adrl/29---main-takeaway/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/29---main-takeaway/</guid>
      <description>&lt;p&gt;The main takeaway from this exploration is that whenever you see a random process converting one distribution to another, SDEs should come to mind as a powerful mathematical framework. The three sampling methods we studied (Langevin dynamics, DDPM, SMLD) are all discrete approximations of continuous SDEs, differing mainly in their drift and diffusion terms. Understanding them through the SDE lens helps unify these approaches and provides a foundation for developing new sampling techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Make Book by Peter Levels</title>
      <link>http://localhost:1313/posts/books/make/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/books/make/</guid>
      <description>&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1520858702i/39165640.jpg&#34; alt=&#34;Make Book Cover&#34; width=&#34;500&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;
&lt;h2 id=&#34;my-takeaways-from-the-book&#34;&gt;My takeaways from the book&lt;/h2&gt;
&lt;h3 id=&#34;record-your-ideas&#34;&gt;Record your ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you do not have idea then you need to live a better life - Be creative. Find problems in your life and solve them - then sell them as a product. Your idea list is very precious. Carry it everywhere. Note down ideas immediately. Ideas can come at any time. Do not miss them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;templates&#34;&gt;Templates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You should have templates for everything - Boiler plate code, launch procedures etc&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;automations&#34;&gt;Automations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use automations exhaustively.&lt;/li&gt;
&lt;li&gt;Only hire humans under dire circumstances otherwise just use automations.&lt;/li&gt;
&lt;li&gt;Do not take external capital. Do not have co-founders - avoid groupthink drama. Most important thing is to not take any descions that may reduce your freedom or having fun.&lt;/li&gt;
&lt;li&gt;Avoid physical products.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;focus-on-core-functionality&#34;&gt;Focus on core functionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Build only the core functionality first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ask-for-money&#34;&gt;Ask for money&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ask people to pay you for your product, do not have ads. Do not sell users data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Masked reconstruction</title>
      <link>http://localhost:1313/posts/adrl/35---masked-reconstruction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/35---masked-reconstruction/</guid>
      <description>&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Contrastive learning is one way to learn representations from unlabeled data. Masked reconstruction is another way.&lt;/p&gt;
&lt;h2 id=&#34;formulation&#34;&gt;Formulation&lt;/h2&gt;
&lt;p&gt;Let us formalize the masked reconstruction framework:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input space $\mathcal{X}$&lt;/li&gt;
&lt;li&gt;A data point $x \in \mathcal{X}$&lt;/li&gt;
&lt;li&gt;A masking operator $\hat{x}$ that corrupts the input&lt;/li&gt;
&lt;li&gt;An encoder-decoder model $T_\theta(x)$ parameterized by $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The process works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply masking operator: $\hat{x} = \text{mask}(x)$&lt;/li&gt;
&lt;li&gt;Pass masked input through model: $\tilde{x} = T_\theta(\hat{x})$&lt;/li&gt;
&lt;li&gt;Reconstruct original input: $\hat{x} \rightarrow \tilde{x} \approx x$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The objective is to minimize reconstruction loss:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum aposteriori estimate(MAP)</title>
      <link>http://localhost:1313/posts/prnn/15---maximum-aposteriori-estimate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/15---maximum-aposteriori-estimate/</guid>
      <description>&lt;p&gt;The Maximum A Posteriori (MAP) estimate is defined as:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$\theta^*_{MAP} = \arg\max_{\theta} p(\theta|V) = \arg\max_{\theta} p(V|\theta) p(\theta)$$
&lt;/div&gt;
where
- $\theta^*_{MAP}$ is the MAP estimate of the parameter $\theta$
- $p(\theta|V)$ is the posterior probability of the parameter $\theta$ given the observed data $V$
- $p(V|\theta)$ is the likelihood of the observed data $V$ given the parameter $\theta$
- $p(\theta)$ is the prior probability of the parameter $\theta$
&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike MLE, MAP estimation incorporates prior knowledge about the parameter $\theta$ and observed data $V$ where as MLE only depends on the observed data $V$.&lt;/li&gt;
&lt;li&gt;MAP provides a balance between the likelihood of the data and the prior beliefs, leading to more robust estimates, especially when the data is limited.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conjugate-prior&#34;&gt;Conjugate prior&lt;/h2&gt;
&lt;p&gt;A conjugate prior is a type of prior distribution $p(\theta)$ such that when multiplied with the likelihood function $p(V|\theta)$ the posterior distribution $p(\theta|V)$ that we get is in the same form as the prior $p(\theta)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Minimizing KL Divergence is Equivalent to Maxmimizing Likelihood</title>
      <link>http://localhost:1313/posts/prnn/7---likelyhood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/7---likelyhood/</guid>
      <description>&lt;p&gt;For a typical ML problem, all we have are samples from the true distribution $P(x)$ ie $data = {(x_i)}_{i=1}^N$ where $x_i \in \mathbb{R}^d$ are data points. We do not know the distribution $P(x)$ explicitly.&lt;/p&gt;
&lt;p&gt;We try our best to estimate the true distribution $P(x)$ by $Q(x; \theta)$ where $\theta$ are the parameters of the model.&lt;/p&gt;
&lt;p&gt;We want to know how well our model $Q(x; \theta)$ is performing. We can do this by calculating the KL divergence between the true distribution $P(x)$ and the estimated distribution $Q(x; \theta)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Miscellaneous Machine Learning Terms</title>
      <link>http://localhost:1313/posts/prnn/27---miscellaneous-terms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/27---miscellaneous-terms/</guid>
      <description>&lt;h2 id=&#34;epoch&#34;&gt;Epoch&lt;/h2&gt;
&lt;p&gt;One complete pass of the entire training dataset for training the model.&lt;/p&gt;
&lt;h2 id=&#34;batch-size&#34;&gt;Batch Size&lt;/h2&gt;
&lt;p&gt;Risk is defined as&lt;/p&gt;
&lt;p&gt;$$L = \frac{1}{N} \sum_{i=1}^N l(y_i, \hat{y}_i)$$&lt;/p&gt;
&lt;p&gt;Where $l(y_i, \hat{y}_i)$ is a general loss function that measures the discrepancy between the true value $y_i$ and the predicted value $\hat{y}_i$ for each sample.&lt;/p&gt;
&lt;p&gt;Instead of calculating the risk over the entire dataset, however calculating the risk over complete dataset is computationally expensive. Hence we calculate the risk over a small subset of the dataset, called a batch to perform back propogation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixture model clustering also known as soft clustering</title>
      <link>http://localhost:1313/posts/prnn/34.1---soft-clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/34.1---soft-clustering/</guid>
      <description>&lt;h2 id=&#34;problem-setup&#34;&gt;Problem Setup&lt;/h2&gt;
&lt;p&gt;Data: Let the dataset be $X = {x_1, x_2, &amp;hellip;, x_N}$, where each data point $x_i \in \mathbb{R}^d$. The data points are unlabeled.&lt;/p&gt;
&lt;p&gt;Latent Variable: Let $z \in {1, &amp;hellip;, m}$ be a latent variable representing the clusters, where $m$ is the total number of clusters.&lt;/p&gt;
&lt;p&gt;We can extend the dataset as $D = {(x_i, z_i)}_{i=1}^N$, where each data point $x_i$ is associated with its (unknown) cluster $z_i$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Motivational links</title>
      <link>http://localhost:1313/posts/motivating-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/motivating-links/</guid>
      <description>&lt;p&gt;These are some things that I stumbled on that motivate me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtube.com/shorts/q1txLXeFyLg?si=0cSsSBFv93QzMN6I&#34;&gt;https://youtube.com/shorts/q1txLXeFyLg?si=0cSsSBFv93QzMN6I&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Multi-Armed Bandit Problem</title>
      <link>http://localhost:1313/posts/reinforcement-learning/2-multi-armed-bandit-problem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/2-multi-armed-bandit-problem/</guid>
      <description>&lt;h2 id=&#34;multi-armed-bandit-problem&#34;&gt;Multi-Armed Bandit Problem&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/content/posts/RL/3..PNG&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;The Multi-Armed Bandit problem is a fundamental concept in reinforcement learning, where an agent must repeatedly select from multiple options, known as &amp;ldquo;arms&amp;rdquo; (akin to slot machines), to maximize its cumulative reward. This scenario is simplified by the fact that the environment is &lt;strong&gt;state-less&lt;/strong&gt;, meaning the outcome of an action is solely dependent on the action itself, without any influence from a changing state.&lt;/p&gt;
&lt;p&gt;Key characteristics of the Multi-Armed Bandit problem include:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Naive GANs</title>
      <link>http://localhost:1313/posts/adrl/6---naive-gans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/6---naive-gans/</guid>
      <description>&lt;p&gt;A Naive GAN (Generative Adversarial Network) refers to the basic or &amp;ldquo;vanilla&amp;rdquo; version of GANs, introduced by Ian Goodfellow in 2014.&lt;/p&gt;
&lt;h2 id=&#34;derivation&#34;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;Recall the genereral expression for the GANs that we got by minimizing F-divergence:&lt;/p&gt;
&lt;div class=&#34;katex-math-block&#34;&gt;
$$\theta^* = \underset{\theta}{\text{argmin}} \underset{\phi}{\sup} \left[\mathbb{E}_{x \sim p_{data}}[T_\phi(x)] - \mathbb{E}_{x \sim p_{generator}}[f^*(T_\phi(x))]\right]$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta$: Parameters of the generator network&lt;/li&gt;
&lt;li&gt;$\phi$: Parameters of the discriminator (critic) network&lt;/li&gt;
&lt;li&gt;$\theta^*$: Optimal parameters for the generator&lt;/li&gt;
&lt;li&gt;$\text{argmin}_\theta$: Argument that minimizes the expression with respect to $\theta$&lt;/li&gt;
&lt;li&gt;$\sup_\phi$: Supremum (least upper bound) with respect to $\phi$&lt;/li&gt;
&lt;li&gt;$T_\phi(x)$: The discriminator function, parameterized by $\phi$&lt;/li&gt;
&lt;li&gt;$f^*$: The convex conjugate of the function $f$ used in the F-divergence&lt;/li&gt;
&lt;li&gt;$p_{data}$: The true data distribution&lt;/li&gt;
&lt;li&gt;$p_{generator}$: The distribution of the generated data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write $T_\phi(x)$ as composite function $T_\phi(x) = \sigma(V_\phi(x))$, where $\sigma$ is the sigmoid function Substitute this in the expression for GANs:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural network</title>
      <link>http://localhost:1313/posts/prnn/25---neural-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/25---neural-network/</guid>
      <description>&lt;p&gt;A neural network is a computational model inspired by the structure and function of biological neural networks. Mathematically, it can be defined as a series of function compositions:&lt;/p&gt;
&lt;p&gt;$$ f(x) = f_L(f_{L-1}(&amp;hellip;f_2(f_1(x)))) $$&lt;/p&gt;
&lt;p&gt;where $L$ is the number of layers in the network, and each function $f_i$ represents a layer operation.&lt;/p&gt;
&lt;p&gt;For a single layer, the operation can be expressed as:&lt;/p&gt;
&lt;p&gt;$$ f_i(x) = \sigma(W_i x + b_i) $$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Noise Contrastive Estimation</title>
      <link>http://localhost:1313/posts/adrl/33---noise-contrastive-estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/33---noise-contrastive-estimation/</guid>
      <description>&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;The core idea behind Noise Contrastive Estimation (NCE) is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Given samples from a distribution, if you know how to distinguish them from noise samples, then you implicitly know the underlying distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you know how to tell what is noise from what is real data, you must have learned something about the true data distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This provides an alternative way to learn probability distributions by transforming the density estimation problem into a binary classification problem between real and noise samples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non parametric density estimation</title>
      <link>http://localhost:1313/posts/prnn/16---non-parametric-density-estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/16---non-parametric-density-estimation/</guid>
      <description>&lt;h2 id=&#34;what-is-parametric-density-estimation&#34;&gt;What is parametric density estimation?&lt;/h2&gt;
&lt;p&gt;In parametric density estimation, we assume that the data is generated from a known distribution, such as the normal distribution, and we estimate the parameters of the distribution using various methods like maximum likelihood estimation or risk minimization.&lt;/p&gt;
&lt;h2 id=&#34;what-is-non-parametric-density-estimation&#34;&gt;What is non-parametric density estimation?&lt;/h2&gt;
&lt;p&gt;However, in non-parametric density estimation, we make no assumptions about the form of the distribution and estimate the density directly from the data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parzen window estimate</title>
      <link>http://localhost:1313/posts/prnn/17---parzen-window-estimate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/17---parzen-window-estimate/</guid>
      <description>&lt;p&gt;Parzen window estimate is also known as the kernel density estimate&lt;/p&gt;
&lt;h2 id=&#34;basic-idea&#34;&gt;Basic idea&lt;/h2&gt;
&lt;p&gt;Recall that we had derived the following equation for non-parametric density estimation:&lt;/p&gt;
&lt;p&gt;$$p(x) = \frac{k}{nV}$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$k$ is the number of data points in region $R$&lt;/li&gt;
&lt;li&gt;$n$ is the total number of data points in the dataset $D$&lt;/li&gt;
&lt;li&gt;$V$ is the volume of the region $R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Parzen window estimate, we fix the volume $V$ and count $k$ by using a window function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Policy Iteration</title>
      <link>http://localhost:1313/posts/reinforcement-learning/06-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/06-feb-2025/</guid>
      <description>&lt;p&gt;In value iteration, we were focused on finding the optimal value function $J^*$. In policy iteration, we start with a random policy and then improve it iteratively.&lt;/p&gt;
&lt;h3 id=&#34;algorithm-description&#34;&gt;Algorithm Description&lt;/h3&gt;
&lt;p&gt;Policy iteration alternates between two main steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Policy Evaluation&lt;/strong&gt;: Given a policy $\mu_k$, compute its cost-to-go vector $J^{\mu_k}$ by solving the system of linear equations:&lt;/p&gt;
&lt;p&gt;$$
J^{\mu_k}(i) = \sum_{j=1}^n p_{ij}(\mu_k(i))(g(i,\mu_k(i),j) + J^{\mu_k}(j)) \quad i=1,\ldots,n
$$&lt;/p&gt;
&lt;p&gt;where $J^{\mu_k}(1), J^{\mu_k}(2), \ldots, J^{\mu_k}(n)$ are the unknowns to be solved for.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Policy iteration for infinite horizon discounted cost problems</title>
      <link>http://localhost:1313/posts/reinforcement-learning/18-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/18-feb-2025/</guid>
      <description>&lt;h2 id=&#34;proposition-optimality-conditions-in-policy-iteration&#34;&gt;Proposition: Optimality Conditions in Policy Iteration&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s examine a fundamental proposition that establishes optimality conditions for policy iteration in infinite horizon discounted cost problems. This proposition provides crucial insights into why policy iteration converges to optimal policies.&lt;/p&gt;
&lt;h3 id=&#34;statement-of-the-proposition&#34;&gt;Statement of the Proposition&lt;/h3&gt;
&lt;p&gt;Consider a stationary policy $\bar{\mu}$ that satisfies the Bellman optimality condition:&lt;/p&gt;
&lt;div class=&#34;math&#34;&gt;
$$
\underbrace{T_{\bar{\mu}}J^{\bar{\mu}} = TJ^{\bar{\mu}}}_{\substack{\text{Policy } \bar{\mu} \text{ achieves the minimum} \\ \text{in the Bellman equation for all states}}}
$$
&lt;/div&gt;
&lt;p&gt;This can be expressed more explicitly for all states $i = 1,2,\ldots,n$ as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Precursor: Stochastic Differential Equations (SDEs)</title>
      <link>http://localhost:1313/posts/adrl/25--precursor-to-stochastic-differential-equations-sdes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/25--precursor-to-stochastic-differential-equations-sdes/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;We explored different sampling methods for generative models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Langevin dynamics (LE):
$$x_{t+1} = x_t + \alpha \nabla_x \log p(x_t) + \sqrt{2\alpha}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DDPM (Denoising Diffusion Probabilistic Models):
$$x_{t+1} = \sqrt{1-\beta_t}x_t + \sqrt{\beta_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SMLD (Score Matching with Langevin Dynamics):
$$x_{t+1} = x_t + \sqrt{\sigma_{t+1}^2 - \sigma_t^2}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A natural question arises: What is the unifying mathematical framework that connects all these sampling methods?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Principal Component Analysis (PCA)</title>
      <link>http://localhost:1313/posts/prnn/36---pca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/36---pca/</guid>
      <description>&lt;h2 id=&#34;problem-setup&#34;&gt;Problem Setup&lt;/h2&gt;
&lt;p&gt;Data: Let the dataset be $X = {x_i}_{i=1}^N$, where each data point $x_i \in \mathbb{R}^d$.&lt;/p&gt;
&lt;p&gt;Goal: We want to reduce the dimensionality of the data from $d$ to $k$ while preserving as much of the data&amp;rsquo;s variance as possible.&lt;/p&gt;
&lt;h2 id=&#34;intuition&#34;&gt;Intuition&lt;/h2&gt;
&lt;p&gt;PCA aims to find a new coordinate system for the data that maximizes the variance along the first coordinate (principal component), while minimizing the variance along the other coordinates. This is useful for data visualization, noise reduction, and feature extraction.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Q Learning</title>
      <link>http://localhost:1313/posts/reinforcement-learning/25-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/25-feb-2025/</guid>
      <description>&lt;p&gt;Q-Learning is a powerful model-free reinforcement learning algorithm that learns the optimal action-value function directly without requiring a model of the environment.&lt;/p&gt;
&lt;h2 id=&#34;understanding-model-based-vs-model-free-approaches&#34;&gt;Understanding Model-Based vs. Model-Free Approaches&lt;/h2&gt;
&lt;h3 id=&#34;model-based-reinforcement-learning&#34;&gt;Model-Based Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;A model of the environment in reinforcement learning refers to explicit knowledge of the system dynamics, specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;State Transition Probabilities&lt;/strong&gt;: $p_{ij}(u)$ - The probability of transitioning from state $i$ to state $j$ when taking action $u$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reward/Cost Functions&lt;/strong&gt;: $g(i,u,j)$ - The immediate reward or cost received when transitioning from state $i$ to state $j$ via action $u$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With a complete model, we can use dynamic programming methods like value iteration or policy iteration to compute optimal policies without direct environment interaction.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularization</title>
      <link>http://localhost:1313/posts/prnn/20---regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/20---regularization/</guid>
      <description>&lt;p&gt;Regularization is a technique to increase model bias to reduce variance by constraining empirical risk minimization.&lt;/p&gt;
&lt;p&gt;regularized empirical risk minimization:&lt;/p&gt;
&lt;p&gt;$$\text{Reg } ERM = \min_{h \in H} \hat{R}(h_\theta) \quad \text{s.t. } \Omega(h_\theta) &amp;lt; k$$&lt;/p&gt;
&lt;p&gt;here $\Omega(h_\theta) &amp;lt; k$ is the regularization function which is a design choice.&lt;/p&gt;
&lt;p&gt;This can be solved using the method of Lagrange multipliers.&lt;/p&gt;
&lt;p&gt;$h(x) = \arg\min_{h_\theta \in H} \left( \hat{R}(h_\theta) + \lambda \Omega(\theta) \right)$&lt;/p&gt;
&lt;p&gt;here $\lambda$ is the regularization parameter which is a design choice.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Risk function (R)</title>
      <link>http://localhost:1313/posts/prnn/3---risk-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/3---risk-function/</guid>
      <description>&lt;p&gt;Recall that the loss function $L(y_i, \hat{y_i})$ measures the error between the predicted label and the true label for a single data point.&lt;/p&gt;
&lt;p&gt;The risk function $R(h)$ is the expected loss over all data points in the dataset. It is defined as:
$$R(h) = \mathbb{E}_{(x,y) \sim P}[L(y, h(x))]$$&lt;/p&gt;
&lt;p&gt;where $P$ is the true data distribution.&lt;/p&gt;
&lt;h2 id=&#34;conditional-risk&#34;&gt;Conditional risk&lt;/h2&gt;
&lt;p&gt;The conditional risk $R(h|x)$ is the expected loss for a given input $x$. It is defined as:
$$R(h(x)|x) = \mathbb{E}_{y \sim P(y|x)}[L(y, h(x))]$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Score matching part 1</title>
      <link>http://localhost:1313/posts/adrl/20---score-matching-part-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/20---score-matching-part-1/</guid>
      <description>&lt;h2 id=&#34;sampling-as-optimization&#34;&gt;Sampling as optimization&lt;/h2&gt;
&lt;p&gt;In probabilistic modeling, sampling can be viewed as an optimization problem where we aim to find the most probable sample from a probability distribution. This can be formulated mathematically as:&lt;/p&gt;
&lt;p&gt;$$x^* = \mathop{\arg \max}_{x} p(x)$$&lt;/p&gt;
&lt;p&gt;Taking the log of the probability (which preserves the optimum due to monotonicity of log):&lt;/p&gt;
&lt;p&gt;$$x^* = \mathop{\arg \max}_{x} \log p(x)$$&lt;/p&gt;
&lt;p&gt;And converting to a minimization problem:&lt;/p&gt;
&lt;p&gt;$$x^* = \mathop{\arg \min}_{x} - \log p(x)$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Score matching part 2</title>
      <link>http://localhost:1313/posts/adrl/21---score-matching-part-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/21---score-matching-part-2/</guid>
      <description>&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$J_{ESM}(\theta) = \mathbb{E}_{p(x)} \left[ \frac{1}{2} | \hat{s}(x; \theta) - \nabla_x \log p(x) |^2 \right]$&lt;/li&gt;
&lt;li&gt;$J_{ISM}(\theta) = \mathbb{E}_{p(x)} \left[ \frac{1}{2} | \hat{s}(x; \theta) |^2 + \text{tr}(\nabla_x \hat{s}(x; \theta)) \right] + C$&lt;/li&gt;
&lt;li&gt;Theorem: $J_{ISM}(\theta) = J_{ESM}(\theta) + C$&lt;/li&gt;
&lt;li&gt;
&lt;div class=&#34;math-katex&#34;&gt;$\theta^* = \mathop{\arg \min}_{\theta} J_{ISM}(\theta)$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;$x_{t+1} = x_t + \alpha \cdot \hat{s}(x_t; \theta^*) + \sqrt{2\alpha} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$&lt;/li&gt;
&lt;li&gt;$\hat{s}(x; \theta)$ modelled using a neural network
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/39.JPG&#34; alt=&#34;Image Description&#34; width=&#34;400&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;there-is-a-problem-with-calculating-the-trace-term&#34;&gt;There is a problem with calculating the trace term&lt;/h2&gt;
&lt;p&gt;The implicit score matching loss function, as we discussed earlier, is given by:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Score matching part 3</title>
      <link>http://localhost:1313/posts/adrl/22---score-matching-part-3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/22---score-matching-part-3/</guid>
      <description>&lt;h2 id=&#34;recap&#34;&gt;Recap&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Langevin equation: $x_{t+1} = x_t + \alpha \cdot \nabla_x \log p(x_t) + \sqrt{2\alpha} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$&lt;/li&gt;
&lt;li&gt;score function: $\nabla_x \log p(x) = s(x)$&lt;/li&gt;
&lt;li&gt;$J_{ESM}(\theta) = \mathbb{E}_{p(x)} \left[ \frac{1}{2} | \hat{s}(x; \theta) - \nabla_x \log p(x) |^2 \right]$&lt;/li&gt;
&lt;li&gt;$J_{ISM}(\theta) = \mathbb{E}_{p(x)} \left[ \frac{1}{2} | \hat{s}(x; \theta) |^2 + \text{tr}(\nabla_x \hat{s}(x; \theta)) \right] + C$&lt;/li&gt;
&lt;li&gt;Theorem: $J_{ISM}(\theta) = J_{ESM}(\theta) + C$&lt;/li&gt;
&lt;li&gt;
&lt;div class=&#34;math-katex&#34;&gt;$J_{PSM}(\theta) = \frac{1}{2} \mathbb{E}_v \mathbb{E}_{p(x)} \left[ \| v^\top \hat{s}_\theta(x) - v^\top s(x) \|^2 \right]$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&#34;math-katex&#34;&gt;$J_{SSM}(\theta) = \mathbb{E}_v \mathbb{E}_{p(x)} \left[ \frac{1}{2}(v^\top \hat{s}_\theta(x))^2 + v^\top (\nabla_x \hat{s}_\theta(x)) v \right]$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Theorem: $J_{PSM}(\theta) = J_{SSM}(\theta) + C$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;big-picture-plan&#34;&gt;Big picture plan&lt;/h2&gt;
  &lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/40.JPG&#34; alt=&#34;Image Description&#34; width=&#34;500&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;h2 id=&#34;denoising-score-matching&#34;&gt;Denoising score matching&lt;/h2&gt;
&lt;p&gt;Denoising score matching (DSM) is an alternative approach to score matching. It introduces an auxiliary variable and works with conditional scores, which offers several advantages.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequence to sequence models (Transformers)</title>
      <link>http://localhost:1313/posts/adrl/transformers/31.5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/transformers/31.5/</guid>
      <description>&lt;p&gt;Let the data be
$$D = {(x_i, y_i)}_{i=1}^N \text{ iid } \sim p(x, y).$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;here $x_i = { x_i^1, x_i^2, \ldots, x_i^k }$, where $x_j^i \in \mathbb{R}^d$ represents a sequence of $k$ vectors of dimension $d$.They are called tokens in usual NLP models.&lt;/li&gt;
&lt;li&gt;here $y_i = { y_i^1, y_i^2, \ldots, y_i^m }$, where $y_j^i \in \mathbb{R}^{d&amp;rsquo;}$ represents a sequence of $m$ vectors of dimension $d&amp;rsquo;$. Note that $d&amp;rsquo; \neq d$. It represents a softmax distribution over a vocabulary of size $d&amp;rsquo;$ in NLP models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The models that map $x_i$ to $y_i$ are called seq2seq models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SMLD as SDE</title>
      <link>http://localhost:1313/posts/adrl/28---smld-as-sde/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/28---smld-as-sde/</guid>
      <description>&lt;h2 id=&#34;step-1-of-big-picture--stochastic-forward-process-to-continuous-sde&#34;&gt;Step 1 of big picture  (Stochastic forward process to continuous SDE)&lt;/h2&gt;
&lt;p&gt;We know SMLD (Score Matching with Langevin Dynamics) forward process is:
$$x_i = x_{i-1} + \sqrt{\sigma_{i}^2 - \sigma_{i-1}^2}z_{i-1}, \quad z_{i-1} \sim \mathcal{N}(0,I)$$&lt;/p&gt;
&lt;p&gt;where $\sigma_i$ is the noise level at step $i$, increasing with $i$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s convert the discrete SMLD process to a continuous SDE:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, let&amp;rsquo;s write the discrete process in terms of continuous time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let $\Delta t = \frac{1}{N}$ be the time step&lt;/li&gt;
&lt;li&gt;Map discrete index $i$ to continuous time $t$: $i = \frac{t}{N}$&lt;/li&gt;
&lt;li&gt;Write $\sigma_i^2 = \sigma(t)^2$ for continuous noise schedule&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The discrete process becomes:
$$x(t + \Delta t) = x(t) + \sqrt{\sigma(t + \Delta t)^2 - \sigma(t)^2}z_t$$
where $z_t \sim \mathcal{N}(0,I)$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Softmax regression also known multiclass regression</title>
      <link>http://localhost:1313/posts/prnn/14---softmax-regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/14---softmax-regression/</guid>
      <description>&lt;p&gt;Let the data be D = {($x_1$, $y_1$), ($x_2$, $y_2$), &amp;hellip;, ($x_n$, $y_n$)}
where $y_i \in {1, 2, &amp;hellip;, K}$ is the multiclass label of the feature vector $x_i$, and $K$ is the number of classes.&lt;/p&gt;
&lt;p&gt;Then the softmax regression model is defined as:
$$P(y_i = k | x_i) = \frac{e^{w_k^T x_i + b_k}}{\sum_{j=1}^K e^{w_j^T x_i + b_j}}$$&lt;/p&gt;
&lt;p&gt;where $w_k$ is the parameter vector for class $k$, and $b_k$ is the bias term for class $k$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some example problems</title>
      <link>http://localhost:1313/posts/adrl/26.1---some-example-problems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/26.1---some-example-problems/</guid>
      <description>&lt;h2 id=&#34;example-1---convert-continuous-sde-to-discrete-soln-without-drift-term&#34;&gt;Example 1 - convert continuous SDE to discrete soln without drift term&lt;/h2&gt;
&lt;p&gt;The continuous stochastic differential equation (SDE) is given by:&lt;/p&gt;
&lt;p&gt;$$dx = \epsilon \sqrt{dt} \quad // \text{cont eqn}$$&lt;/p&gt;
&lt;p&gt;where $\epsilon \sim \mathcal{N}(0,1)$.&lt;/p&gt;
&lt;p&gt;To convert this continuous SDE to a discrete solution, we consider the increments:&lt;/p&gt;
&lt;p&gt;$$X_{t_{i+1}} - X_{t_i} = \sqrt{t_{i+1} - t_i} \cdot \epsilon$$&lt;/p&gt;
&lt;p&gt;Assuming $t$ were not discrete, we can express it as:&lt;/p&gt;
&lt;p&gt;$$X_{t_{i+1}} = X_{t_i} + \sqrt{t_{i+1} - t_i} \cdot \epsilon$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>State space models &amp; Mamba</title>
      <link>http://localhost:1313/posts/adrl/32-state-space-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/32-state-space-models/</guid>
      <description>&lt;h2 id=&#34;motivation-for-state-space-models&#34;&gt;Motivation for state space models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RNNs can handle variable length inputs however they are not parallelizable.&lt;/li&gt;
&lt;li&gt;CNNs are parallelizable however they cannot handle variable length inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Can we get the best of both worlds?&lt;/p&gt;
&lt;h2 id=&#34;linear-state-space-models&#34;&gt;Linear State Space Models&lt;/h2&gt;
&lt;p&gt;A linear state space model consists of two equations:&lt;/p&gt;
&lt;p&gt;$$h&amp;rsquo;(t) = Ah(t) + Bx(t)$$
$$y(t) = Ch(t) + Dx(t)$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h(t)$ is the hidden state at time $t$&lt;/li&gt;
&lt;li&gt;$x(t)$ is the input at time $t$&lt;/li&gt;
&lt;li&gt;$y(t)$ is the output at time $t$&lt;/li&gt;
&lt;li&gt;$h&amp;rsquo;(t)$ is the derivative of the hidden state with respect to time&lt;/li&gt;
&lt;li&gt;$A, B, C, D$ are learnable parameter matrices that define the dynamics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first equation describes how the hidden state evolves over time based on:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stochastic Differential Equations (SDEs)</title>
      <link>http://localhost:1313/posts/adrl/26---stochastic-differential-equations-sdes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/26---stochastic-differential-equations-sdes/</guid>
      <description>&lt;h2 id=&#34;big-picture-of-what-we-are-going-to-do&#34;&gt;Big picture of what we are going to do:&lt;/h2&gt;
  &lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/42.PNG&#34; alt=&#34;Image Description&#34; width=&#34;800&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;h2 id=&#34;big-picture-of-sdes&#34;&gt;Big picture of SDEs&lt;/h2&gt;
&lt;p&gt;This topic comes under stochastic calculus, which deals with functions whose derivatives yield random vectors rather than deterministic points.&lt;/p&gt;
&lt;p&gt;In traditional calculus, when we take the derivative of a function at a point, we get a fixed value ie a scalar that represents the instantaneous rate of change. However, in stochastic calculus, the derivative at any point is a random variable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Summarizing DDPM</title>
      <link>http://localhost:1313/posts/adrl/19---summarizing-ddpm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/19---summarizing-ddpm/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;VAE:
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/38.JPG&#34; alt=&#34;Image Description&#34; width=&#34;200&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;/li&gt;
&lt;li&gt;Hierarchical VAE:
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/36.JPG&#34; alt=&#34;Image Description&#34; width=&#34;600&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;/li&gt;
&lt;li&gt;DDPM(Hierarchical VAE with deterministic forward process):
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/37.JPG&#34; alt=&#34;Image Description&#34; width=&#34;600&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt; 
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that $\phi$ has been removed from $q_\phi()$.&lt;/p&gt;
&lt;p&gt;Also see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ELBO for a Variational Autoencoder (VAE) is given by:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$
\text{ELBO (VAE)} = \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x|z)}{q_\phi(z|x)} \right]
$$
&lt;/div&gt;
- For Denoising Diffusion Models (DDPM), the ELBO is expressed as:
&lt;div class=&#34;math-katex&#34;&gt;
$$
\text{ELBO (DM)} = \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)} \right]
$$
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Support measure machine (SCM) as Empirical Risk Minimization (ERM)</title>
      <link>http://localhost:1313/posts/prnn/24---scm-as-erm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/24---scm-as-erm/</guid>
      <description>&lt;p&gt;Recall that in support measure machine (SCM), we were trying to maximize the margin between the two classes of data points.&lt;/p&gt;
&lt;p&gt;$$ \min_{w,b,\xi} \frac{1}{2} |w|^2 + C \sum_{i=1}^n \xi_i $$
$$ \text{s.t. } y_i (w^T x_i + b) \geq 1 - \xi_i \text{ for all } i $$
$$ \xi_i \geq 0 \text{ for all } i $$&lt;/p&gt;
&lt;h2 id=&#34;scm-as-erm&#34;&gt;SCM as ERM&lt;/h2&gt;
&lt;p&gt;This is an optimization problem that can be formulated as a empirical risk minimization (ERM) problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support vector machine(SVM) when data is linearly separable</title>
      <link>http://localhost:1313/posts/prnn/21---support-vector-machine---data-linearly-separable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/21---support-vector-machine---data-linearly-separable/</guid>
      <description>&lt;h2 id=&#34;dataset-is-linearly-separable&#34;&gt;Dataset is linearly separable&lt;/h2&gt;
&lt;p&gt;Dataset $D = {(x_i, y_i)}_{i=1}^n$ where $x_i \in \mathbb{R}^d$ and $y_i \in {-1, 1}$ is said to be linearly separable if there exists a hyperplane that can separate the two classes of data points with zero training error.&lt;/p&gt;
&lt;p&gt;Mathematically, a dataset is linearly separable if there exist weights $w$ and bias $b$ such that:
$$ w^T x_i + b &amp;gt; 1 \text{ for } y_i = 1 $$
$$ w^T x_i + b &amp;lt; -1 \text{ for } y_i = -1 $$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support vector machine(SVM) when data is not linearly separable</title>
      <link>http://localhost:1313/posts/prnn/22---svm---dataset-is-not-linearly-separable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/22---svm---dataset-is-not-linearly-separable/</guid>
      <description>&lt;h2 id=&#34;dataset-is-not-linearly-separable&#34;&gt;Dataset is not linearly separable&lt;/h2&gt;
&lt;p&gt;Unlike the case when the dataset is linearly separable, we cannot find a hyperplane that separates the two classes of data points with zero training error.&lt;/p&gt;
&lt;h2 id=&#34;soft-margin&#34;&gt;Soft margin&lt;/h2&gt;
&lt;p&gt;To handle this case, we introduce a slack variable $\xi_i$ for each data point $x_i$ to allow some points to be on the wrong side of the margin or even in the wrong class.&lt;/p&gt;
&lt;h2 id=&#34;optimization-problem&#34;&gt;Optimization problem&lt;/h2&gt;
&lt;p&gt;The optimization problem becomes:&lt;/p&gt;</description>
    </item>
    <item>
      <title>SVM with kernel</title>
      <link>http://localhost:1313/posts/prnn/23---svm-with-kernel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/23---svm-with-kernel/</guid>
      <description>&lt;h2 id=&#34;prerequisites---kernel-trick&#34;&gt;Prerequisites - Kernel trick&lt;/h2&gt;
&lt;p&gt;Kernel function $k(x, y) = \phi(x)^T \phi(y)$ where $\phi(x)$ is a feature mapping that maps the data points to a higher dimensional space without actually computing the feature mapping, i.e., $\phi(x)$.&lt;/p&gt;
&lt;p&gt;In other words, we can compute the kernel function $k(x, y)$ without actually computing the feature mapping $\phi(x)$.&lt;/p&gt;
&lt;p&gt;Examples of kernel functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomial kernel: $k(x_1, x_2) = \phi(x_1)^T \phi(x_2) = (1 + x_1^T x_2)^p$&lt;/li&gt;
&lt;li&gt;Sigmoid kernel: $k_s(x_1, x_2) = \frac{1}{1 + \exp(a x_1^T x_2)}$&lt;/li&gt;
&lt;li&gt;RBF/Gaussian kernel: $k(x_1, x_2) = \exp\left(-\frac{| x_1 - x_2 |^2}{\sigma^2}\right)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;If the dataset $D = {(x_i, y_i)}_{i=1}^n$ where $x_i \in \mathbb{R}^d$ and $y_i \in {-1, 1}$ is not linearly separable in the original $d$-dimensional space, we can use a feature mapping $\phi(x)$ to map the data points to a higher dimensional space where they become linearly separable and then use the SVM optimization problem to find the optimal hyperplane.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TD Learning</title>
      <link>http://localhost:1313/posts/reinforcement-learning/20-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/20-feb-2025/</guid>
      <description>&lt;p&gt;Temporal Difference (TD) learning, introduced by Rich Sutton in 1984, is a fundamental reinforcement learning method that combines ideas from Monte Carlo methods and dynamic programming.&lt;/p&gt;
&lt;h3 id=&#34;recap-of-monte-carlo-methods&#34;&gt;Recap of Monte Carlo Methods&lt;/h3&gt;
&lt;p&gt;Recall that Monte Carlo methods estimate the value function by:&lt;/p&gt;
&lt;div class=&#34;math&#34;&gt;
$$
V_\pi(s) = E[G_n | S_0 = s]
$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G_n = r_{n+1} + r_{n+2} + &amp;hellip; + r_N$ is the return (sum of rewards until terminal state)&lt;/li&gt;
&lt;li&gt;$N$ is the terminal instant of the episode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monte Carlo methods work with sample average data collected over complete trajectories, requiring episodes to terminate before learning can occur.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Temporal difference learning</title>
      <link>http://localhost:1313/posts/reinforcement-learning/13-mar-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/13-mar-2025/</guid>
      <description>&lt;p&gt;Temporal Difference (TD) learning is a fundamental reinforcement learning technique that combines ideas from Monte Carlo methods and dynamic programming. It allows agents to learn from experience without requiring a model of the environment, making it particularly valuable for real-world applications.&lt;/p&gt;
&lt;h2 id=&#34;understanding-policy-evaluation&#34;&gt;Understanding Policy Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;mathematical-foundation&#34;&gt;Mathematical Foundation&lt;/h3&gt;
&lt;p&gt;Policy evaluation refers to the process of determining the value function $J_π$ for a given policy $π$. The value function represents the expected cumulative reward when following policy $π$ from a given state.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two types of neural networks</title>
      <link>http://localhost:1313/posts/adrl/12.1---two-types-of-nns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/12.1---two-types-of-nns/</guid>
      <description>&lt;p&gt;We can model distributions using neural networks in two ways:&lt;/p&gt;
&lt;h2 id=&#34;deterministic-way&#34;&gt;Deterministic way&lt;/h2&gt;
&lt;p&gt;In this way the output of neural network is a sample from the distribution.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/21.JPG&#34; alt=&#34;Image Description&#34; width=&#34;500&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;
&lt;h2 id=&#34;probabilistic-way&#34;&gt;Probabilistic way&lt;/h2&gt;
&lt;p&gt;In this way the output of neural network are parameters of the distribution.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/22.JPG&#34; alt=&#34;Image Description&#34; width=&#34;600&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Understanding Stochastic Shortest Path Problems: A Deep Dive into Mathematical Foundations</title>
      <link>http://localhost:1313/posts/reinforcement-learning/30-jan-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/30-jan-2025/</guid>
      <description>&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;h3 id=&#34;expected-single-stage-cost&#34;&gt;Expected Single-Stage Cost&lt;/h3&gt;
&lt;p&gt;The expected single-stage cost under policy $\mu$ in state $x$, denoted as $\bar{g}(x,\mu(x))$, is:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
$$
\underbrace{\bar{g}(x,\mu(x))}_{\substack{\text{Expected} \\ \text{single-stage cost}}} = \sum_{j=0}^n \underbrace{p_{xj}(\mu(x))}_{\substack{\text{Transition} \\ \text{probability}}} \underbrace{g(x,\mu(x),j)}_{\substack{\text{Stage cost for} \\ \text{this transition}}}
$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_{xj}(\mu(x))$ is the probability of transitioning from state $x$ to state $j$ under action $\mu(x)$&lt;/li&gt;
&lt;li&gt;$g(x,\mu(x),j)$ is the stage cost for this transition&lt;/li&gt;
&lt;li&gt;The sum is over all possible next states $j$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This represents the immediate expected cost when taking the action prescribed by policy $\mu$ in state $x$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unsupervised learning</title>
      <link>http://localhost:1313/posts/prnn/34---unsupervised-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/34---unsupervised-learning/</guid>
      <description>&lt;p&gt;In unsupervised learning, we don&amp;rsquo;t have labeled data. We want to find structure in the data.
Hence the data is of the form $X = {x_1, &amp;hellip;, x_N}$ where $x_i \in \mathbb{R}^d$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VAE as regularized autoencoder</title>
      <link>http://localhost:1313/posts/adrl/14---vae-as-regularized-autoencoder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/14---vae-as-regularized-autoencoder/</guid>
      <description>&lt;h2 id=&#34;understanding-vae-as-a-regularized-autoencoder&#34;&gt;Understanding VAE as a Regularized Autoencoder&lt;/h2&gt;
&lt;p&gt;Recall that the Variational Autoencoder (VAE) objective is given by:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$ F_{\theta}(q) = \mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right] - D_{KL} \left( q_{\phi}(z|x) \| p(z) \right) $$
&lt;/div&gt;
&lt;p&gt;We can rewrite this as:&lt;/p&gt;
&lt;div class=&#34;math-katex&#34;&gt;
$$ F_{\theta}(q) = \mathbb{E}_{q_{\phi}(z|x)} \left[ \|x - \hat{x}\|^2 \right] - D_{KL} \left( q_{\phi}(z|x) \| p(z) \right) $$
&lt;/div&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first term is the reconstruction error&lt;/li&gt;
&lt;li&gt;The second term acts as a regularizer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This formulation shows that the VAE objective can be interpreted as a regularized autoencoder, where the regularization term encourages the latent distribution to be close to the prior $p(z)$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Value Iteration</title>
      <link>http://localhost:1313/posts/reinforcement-learning/04-feb-2025/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/reinforcement-learning/04-feb-2025/</guid>
      <description>&lt;p&gt;Value iteration is a numerical method for solving Stochastic Shortest Path (SSP) problems that relies on the dynamic programming operator $T_\mu$. Let&amp;rsquo;s examine how this operator works and its key properties:&lt;/p&gt;
&lt;h3 id=&#34;the-dynamic-programming-operator-t_mu&#34;&gt;The Dynamic Programming Operator $T_\mu$&lt;/h3&gt;
&lt;p&gt;For a given policy $\mu$, the dynamic programming operator $T_\mu$ transforms one value function into another. Mathematically, for any state $x$, we have:&lt;/p&gt;
&lt;div class=&#34;math-block&#34;&gt;
$$
\underbrace{(T_\mu J)(x)}_{\substack{\text{New value function} \\ \text{at state } x}} = \underbrace{E_{x_{k+1}}}_{\substack{\text{Expected value over} \\ \text{next states}}} \left[ \underbrace{g(x,\mu(x),x_{k+1})}_{\substack{\text{Stage cost under} \\ \text{policy } \mu}} + \underbrace{J(x_{k+1})}_{\substack{\text{Cost-to-go from} \\ \text{next state}}} \biggm\vert x_k=x \right]
$$
&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational encoding(VAEs)</title>
      <link>http://localhost:1313/posts/adrl/12---variational-encoding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/12---variational-encoding/</guid>
      <description>&lt;h2 id=&#34;problem-setting&#34;&gt;Problem setting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Given a set of data points $D = ({x_i})_{i=1}^N$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Where $x_i \in \mathbb{R}^d$ are iid samples from some unknown distribution $p_{\text{data}}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We want to model $p_{\text{data}}$ as $p_{\theta}$ using a neural network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We model $p_{\theta}$ as a latent variable model&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;modelling&#34;&gt;Modelling&lt;/h2&gt;
&lt;p&gt;Log likelihood is given by:&lt;/p&gt;
&lt;p&gt;$$ \ell(\theta) = \log p_{\theta}(x) $$&lt;/p&gt;
&lt;p&gt;We assume that each data point $x_i$ is associated with a latent variable $z_i$.
Hence, we will introduce the latent variable $z$ and marginalize over it:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vector Quantized Variational Autoencoders (VQ-VAEs)</title>
      <link>http://localhost:1313/posts/adrl/15---vqvae/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/15---vqvae/</guid>
      <description>&lt;p&gt;Vector Quantized Variational Autoencoders (VQ-VAEs) were introduced to address several limitations of regular VAEs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &amp;ldquo;posterior collapse&amp;rdquo; problem where the latent code is ignored&lt;/li&gt;
&lt;li&gt;The continuous latent space can make it difficult to model discrete structures&lt;/li&gt;
&lt;li&gt;The aggregated posterior mismatch between $q_\phi(z)$ and the prior $p(z)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;VQ-VAEs solve these issues by using discrete latent variables through vector quantization. The architecture consists of:&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/victor-explore/ADRL-Notes/refs/heads/main/28.JPG&#34; alt=&#34;Vector Quantized Variational Autoencoder Architecture&#34; width=&#34;900&#34; height=&#34;auto&#34;/&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An encoder network $E_{\phi}$ that maps input $x \in \mathbb{R}^N$ to continuous latent vectors $z_e = E_{\phi}(x) \in \mathbb{R}^{D}$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wasserstein GANs (WGANs)</title>
      <link>http://localhost:1313/posts/adrl/8---wasserstein-gans-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/8---wasserstein-gans-/</guid>
      <description>&lt;h2 id=&#34;perfect-discriminator-theorem&#34;&gt;Perfect Discriminator theorem&lt;/h2&gt;
&lt;p&gt;The Perfect Discriminator theorem states that if two distributions $p_x$ and $q_x$ have support on two disjoint subsets $M$ and $P$ respectively, there always exists a discriminator $D^*: x \rightarrow [0,1]$ that has accuracy 1:&lt;/p&gt;
&lt;p&gt;$$\forall x \in M \cup P, D^*(x) = \begin{cases} 1 &amp;amp; \text{if } x \in M \ 0 &amp;amp; \text{if } x \in P \end{cases}$$&lt;/p&gt;
&lt;p&gt;This discriminator achieves perfect classification, as it correctly identifies the origin of every sample with 100% accuracy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wasserstein metric</title>
      <link>http://localhost:1313/posts/adrl/7--wasserstein-metric/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/adrl/7--wasserstein-metric/</guid>
      <description>&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;We can use Wasserstein metric in place of F-divergence to define a distance between two distributions $P$ and $Q$ both defined over the same space $X$.&lt;/p&gt;
&lt;p&gt;The Wasserstein distance $W_p(P,Q)$ of order $p$ is defined as:&lt;/p&gt;
&lt;p&gt;$$W_p(P,Q) = \left( \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(x,y) \sim \gamma} [d(x,y)^p] \right)^{1/p}$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Gamma(P,Q)$ is the set of all possible joint distributions of $(x,y)$ where $x,y \in X$ with marginals $x \sim P$ and $y \sim Q$ (also called couplings)&lt;/li&gt;
&lt;li&gt;$\gamma(x,y)$ is a particular joint distribution chosen out of all possible joint distributions in $\Gamma(P,Q)$ that minimizes $\mathbb{E}_{(x,y) \sim \gamma} [d(x,y)^p]$&lt;/li&gt;
&lt;li&gt;$d(x,y)$ is the metric (distance) between points $x$ and $y$ in the space $X$&lt;/li&gt;
&lt;li&gt;$p \geq 1$ is the order of the Wasserstein distance. Common choices are $p=1$ (first-order Wasserstein distance) and $p=2$ (second-order Wasserstein distance).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;properties-of-wasserstein-metric&#34;&gt;Properties of Wasserstein metric&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;When $p=1$, the Wasserstein distance is equivalent to the Earth Mover&amp;rsquo;s Distance (EMD), because it can be interpreted as the minimum amount of work required to transform one distribution into another.&lt;/li&gt;
&lt;li&gt;When $p=2$, the Wasserstein distance is equivalent to the squared Euclidean distance.&lt;/li&gt;
&lt;li&gt;The Wasserstein distance is always finite, unlike the F-divergence, which can be infinite.&lt;/li&gt;
&lt;li&gt;The Wasserstein distance is a true metric, meaning it satisfies the triangle inequality.&lt;/li&gt;
&lt;li&gt;When KL divergence is $0$ then Wasserstein distance is $0$, but not vice versa.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;proof-for-5th-property---kl-divergence-is-0-then-wasserstein-distance-is-0&#34;&gt;Proof for 5th property - KL divergence is 0 then Wasserstein distance is 0&lt;/h2&gt;
&lt;p&gt;To prove that when the KL divergence is 0, the Wasserstein distance is also 0, we start with the definitions of both metrics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What should i do if AI is improving</title>
      <link>http://localhost:1313/posts/what-should-i-do-if-ai-is-improving/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/what-should-i-do-if-ai-is-improving/</guid>
      <description>&lt;h1 id=&#34;given-that&#34;&gt;Given that&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Seems that AI is improving at a rapid pace&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;following-jobs-will-be-lost&#34;&gt;Following jobs will be lost&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Text to video models etc will replace Actors, producers, writers etc.&lt;/li&gt;
&lt;li&gt;Text to speech models will replace Singers&lt;/li&gt;
&lt;li&gt;AI pimps will replace Only fans, Ticktok creators etc&lt;/li&gt;
&lt;li&gt;Tools like Claude computer use will replace all remote workers&lt;/li&gt;
&lt;li&gt;Indie hackers relying on microsaas will be replaced by AI. Infact most SaaS will be replaced by AI.&lt;/li&gt;
&lt;li&gt;Bloggers and most sites will be replaced by SearchGPTs.&lt;/li&gt;
&lt;li&gt;All consultants like lawyers who do not go to court, accountants etc.&lt;/li&gt;
&lt;li&gt;I cannot find a job where you do 9 to 5 on a computer and you not getting replaced by AI.&lt;/li&gt;
&lt;li&gt;99% of all book authors except for people like Marcus Aurelius will be replaced by AI.&lt;/li&gt;
&lt;li&gt;Online courses, teachers even nice people like Khan Academy will be replaced by AI. Heck i do not think that even great colleges like IISC and IITs teaching computer science will be spared.&lt;/li&gt;
&lt;li&gt;Copywriters&lt;/li&gt;
&lt;li&gt;Affiliate marketers because of tools like perplexity shopping&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;if-we-are-able-to-build-robots&#34;&gt;If we are able to build robots&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;If we are able to get robots which also seems likely then even jobs that require physical actions will also be replaced by AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-to-prepare-for-this-brave-new-world&#34;&gt;How to prepare for this brave new world&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Invest all your money to buy AI tools and tech upgrades.&lt;/li&gt;
&lt;li&gt;Be a super learner.&lt;/li&gt;
&lt;li&gt;Obviously it will become easier to make a software/videos/agents etc in the future but you still have to push the current tools to absolute limits - so that you are prepared for the future when these tools mature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;these-effors-will-not-be-in-vain&#34;&gt;These effors will not be in vain&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Building an audience on X.&lt;/li&gt;
&lt;li&gt;Keeping up with AI progress. Learning about AI.&lt;/li&gt;
&lt;li&gt;Evergreen phylosophy etc&lt;/li&gt;
&lt;li&gt;Spending time with family and friends.&lt;/li&gt;
&lt;li&gt;I building AI agents will be the only thing that matters if there was one thing that mattered.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-i-think-will-still-be-valuable&#34;&gt;What i think will still be valuable&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bitcoin, Ethereum maybe&lt;/li&gt;
&lt;li&gt;Land&lt;/li&gt;
&lt;li&gt;I will not bet that thousand year old trend of gold being valueable will be replaced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;i-feel-that-somehow-these-jobs-will-still-be-there&#34;&gt;I feel that somehow these jobs will still be there&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Sports&lt;/li&gt;
&lt;li&gt;I would have written artists, actors etc but most of them are crooks in real life. We do not watch them because we are inspired by them in real life.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;contarary-to-popular-belief&#34;&gt;Contarary to popular belief&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;I think AI will be able to empatize even better than humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;these-are-the-things-that-excite-me&#34;&gt;These are the things that excite me&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;AR glasses with a personal coach/secretary.&lt;/li&gt;
&lt;li&gt;GPT 5/6 level AI agents.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Why we need a virtual environment</title>
      <link>http://localhost:1313/posts/virtual-environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/virtual-environment/</guid>
      <description>&lt;h2 id=&#34;what-is-a-programming-language&#34;&gt;What is a Programming Language?&lt;/h2&gt;
&lt;p&gt;A programming language is a formal system that allows humans to give instructions to computers. It acts as a bridge between human-readable commands and machine code. Python, which we&amp;rsquo;ll focus on in this article, is a popular high-level programming language known for its readable syntax and extensive library ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-module&#34;&gt;What is a Module?&lt;/h2&gt;
&lt;p&gt;A module in Python is a file containing Python code that can be imported and used in other Python programs. It&amp;rsquo;s a fundamental way to organize and reuse code by grouping related functions, classes, and variables together.&lt;/p&gt;</description>
    </item>
    <item>
      <title>XGBoost - Gradient boosted regression tree</title>
      <link>http://localhost:1313/posts/prnn/32---xg-boost/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/prnn/32---xg-boost/</guid>
      <description>&lt;p&gt;In XGBoost, the weak learners are decision trees.&lt;/p&gt;
&lt;p&gt;The algorithm builds these trees sequentially, with each new tree aiming to correct the errors of the combined previous trees.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Recall that $r_i = -\frac{\partial R(H_{T-1}(x_i))}{\partial H_{T-1}(x_i)}$ are the pseudo-residuals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;XGBoost formulates the optimization problem for finding the next weak learner as:&lt;/p&gt;
&lt;p&gt;$$ h_T = \arg\min_{h \in H} \sum_{i=1}^n (r_i \cdot h(x_i) + \frac{1}{2} h(x_i)^2) $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can rewrite this by defining $\hat{y}_i = -r_i$:&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/posts/motivating-links-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/motivating-links-copy/</guid>
      <description></description>
    </item>
  </channel>
</rss>
