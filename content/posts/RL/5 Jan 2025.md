---
title: "Class 1 - Introduction to Reinforcement Learning"
date:
draft: true
description:
tags: []
categories: []
author:
toc:
weight: 1
---


## Core Idea of Reinforcement Learning

Reinforcement Learning (RL) is a computational approach to learning whereby an **agent** seeks to maximize some notion of cumulative reward by taking actions in an **environment**. This process is akin to training a dog to perform a trick: you cannot dictate every move, but you can provide rewards when it performs correctly. In RL, the agent learns through trial and error, receiving feedback in the form of **rewards** or penalties based on its **actions**. The primary objective is to develop a **policy**, a strategy that guides the agent to take actions that maximize the total accumulated reward over time.

![](/content/posts/RL/1.PNG)

Let's rigorously define the key components of a Reinforcement Learning system:

- **Agent**: The decision-making entity that interacts with and learns from the environment. The agent observes states, takes actions, receives rewards, and updates its policy to improve its decision-making over time. Formally, it is defined by its policy $\pi$ and value functions.

- **Environment**: The external system with which the agent interacts. It is characterized by:
  - A state space $\mathcal{S}$ containing all possible states.
  - An action space $\mathcal{A}$ containing all possible actions.
  - A transition function $P(s'|s,a)$ defining the probability of transitioning to state $s'$ given current state $s$ and action $a$.
  - A reward function $R(s,a,s')$ defining the reward received when transitioning from $s$ to $s'$ via action $a$.

- **State ($S_t \in \mathcal{S}$)**: A complete description of the environment at time $t$ that contains all information relevant for future decision making. The state must satisfy the Markov property:

  $$P(S_{t+1}|S_t,A_t) = P(S_{t+1}|S_t,A_t,S_{t-1},A_{t-1},...,S_0,A_0)$$

  This means the current state captures all relevant information from the history.

- **Action ($A_t \in \mathcal{A}$)**: A choice made by the agent at time $t$ that influences the environment. Actions are selected according to the agent's policy $\pi$ and can be:
  - Discrete: $\mathcal{A} = \{a_1, a_2, ..., a_n\}$ 
  - Continuous: $\mathcal{A} \subseteq \mathbb{R}^n$

- **Reward ($R_t \in \mathbb{R}$)**: The immediate feedback signal received after taking action $A_{t-1}$ in state $S_{t-1}$. The reward function $R: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ defines the reward dynamics:
  $R_t = R(S_{t-1}, A_{t-1}, S_t)$

- **Policy ($\pi$)**: The agent's decision-making strategy that maps states to actions. Two types exist:
  - **Deterministic Policy**: $\pi: \mathcal{S} \rightarrow \mathcal{A}$
    Directly maps each state to an action: $a = \pi(s)$
  - **Stochastic Policy**: $\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$
    Maps states to probability distributions over actions: $\pi(a|s) = P(A_t=a|S_t=s)$
    Must satisfy: $\sum_{a \in \mathcal{A}} \pi(a|s) = 1, \forall s \in \mathcal{S}$

![](/content/posts/RL/2..PNG)

- **Goal and Optimal Policy**: The agent aims to find an optimal policy $\pi^*$ that maximizes the expected cumulative reward (return) $G_t$:
  $G_t = \sum_{k=0}^{T-t} \gamma^k R_{t+k+1}$
  where $\gamma \in [0,1]$ is a discount factor that determines the relative importance of immediate versus future rewards.
  The optimal policy satisfies: $\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}[G_t|S_t=s], \forall s \in \mathcal{S}$

- **Value Functions**: Two key functions quantify the expected return:
  - **State-Value Function**: $V_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]$
    Expected return starting from state $s$ and following policy $\pi$
  - **Action-Value Function**: $Q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]$
    Expected return starting from state $s$, taking action $a$, then following policy $\pi$
  These functions satisfy the Bellman equations, fundamental to RL algorithms.

- **Horizon ($T$)**: The time span of agent-environment interaction:
  - **Finite Horizon**: Fixed number of steps $T < \infty$
  - **Infinite Horizon**: $T = \infty$, typically with discounting ($\gamma < 1$)
  - **Episodic**: Tasks that naturally terminate after variable length sequences

## Core Problems in Reinforcement Learning
In the field of Reinforcement Learning, we encounter two primary types of problems that are essential for understanding and developing effective learning algorithms:

- **Prediction Problem**: Formally, given a policy $\pi$, the prediction problem, also known as policy evaluation, involves determining the value function $V_{\pi}$ or the action-value function $Q_{\pi}$. Mathematically, the value function $V_{\pi}(s)$ is defined as:

  $$ V_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] $$

  where $G_t$ is the return, representing the cumulative future rewards. The prediction problem asks: "What is the expected return if the agent follows this policy $\pi$ from state $s$?" This evaluation helps us understand the effectiveness of a given policy by quantifying the expected rewards.

- **Control Problem**: The control problem focuses on finding the optimal policy $\pi^*$ that maximizes the value function $V_{\pi^*}$ or the action-value function $Q_{\pi^*}$. The goal is to identify the best strategy for the agent to maximize its cumulative reward. Mathematically, the optimal policy $\pi^*$ satisfies:

  $$ \pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}[G_t | S_t = s], \forall s \in \mathcal{S} $$

  Solving the control problem typically involves iteratively addressing the prediction problem to evaluate the current policy and then improving the policy based on these evaluations. This iterative process is crucial for refining the agent's strategy to achieve optimal performance.

In summary, reinforcement learning fundamentally aims to solve the control problem, with the prediction problem serving as a critical step in evaluating and improving policies to reach optimal decision-making strategies.

## Multi-Armed Bandit Problem

![](/content/posts/RL/3..PNG)

The Multi-Armed Bandit problem is a fundamental concept in reinforcement learning, where an agent must repeatedly select from multiple options, known as "arms" (akin to slot machines), to maximize its cumulative reward. This scenario is simplified by the fact that the environment is **state-less**, meaning the outcome of an action is solely dependent on the action itself, without any influence from a changing state.

Key characteristics of the Multi-Armed Bandit problem include:

- **State-less Environment**: The environment lacks dynamic states that evolve over time, meaning the agent's decision at each step is unaffected by previous states of the environment.
- **Multiple Actions (Arms)**: The agent has a choice among $k$ distinct actions, referred to as arms, represented by the set $\{1, 2, \ldots, k\}$.
- **Unknown Reward Distributions**: Each arm $a$ is linked to an unknown probability distribution of rewards. When the agent selects arm $a$ at time step $t$ (denoted as $A_t = a$), it receives a reward $R_{t+1}$ drawn from the distribution associated with that arm. The agent is initially unaware of these distributions.
- **Objective**: The agent's goal is to learn a strategy for selecting actions over time that maximizes the total expected reward accumulated across a series of trials. This requires balancing exploration (testing different arms to understand their reward distributions) and exploitation (choosing the arm currently believed to yield the highest reward based on existing knowledge).

To formalize this, we define the **true value of an action** $a$, denoted as $q^*(a)$, which is the expected reward when action $a$ is chosen. Mathematically, this is expressed as the expected value of the reward $R_{t+1}$ given that action $A_t = a$ was taken:

$$ q^*(a) = \mathbb{E}[R_{t+1} | A_t = a] $$

Here, $\mathbb{E}[\cdot]$ denotes the expected value. Intuitively, $q^*(a)$ represents the average reward anticipated from repeatedly selecting arm $a$.

The **optimal action**, denoted as $a^*$, is the action with the highest expected reward. The objective is to identify this optimal action:

$$ a^* = \arg\max_{a} q^*(a) $$

where $\arg\max_{a}$ signifies "the action $a$ that maximizes the expression". In this context, $q^*(a)$ functions as the **action-value function**, which measures the desirability or "value" of taking action $a$, reflecting the expected long-term reward from that action. Since the true reward distributions are unknown, these action values must be estimated through experience.

### Action-value Methods

Action-value methods are crucial for estimating the expected reward of taking a specific action $a$, denoted as $q^*(a)$. This expected reward is not known a priori, so we estimate it through experience by averaging the rewards obtained from taking action $a$.

Define $q_n(a)$ as the estimated action-value for action $a$ after $n$ observations. The formula for $q_n(a)$ is:

$$ q_n(a) = \frac{\sum_{i=1}^{n} R_i \cdot \mathbb{I}\{A_{i-1} = a\}}{\sum_{i=1}^{n} \mathbb{I}\{A_{i-1} = a\}} $$

Breaking down the formula:

- **$q_n(a)$**: The estimated action-value for action $a$ based on experience.

- **$R_i$**: The reward received at time step $i$, which is the feedback from the environment after taking action $A_{i-1}$.

- **$A_{i-1}$**: The action taken at the previous time step ($i-1$).

- **$\mathbb{I}\{A_{i-1} = a\}$**: An indicator function that equals 1 if action $a$ was taken at time step $i-1$, and 0 otherwise.

- **Numerator: $\sum_{i=1}^{n} R_i \cdot \mathbb{I}\{A_{i-1} = a\}$**: This sums all rewards received when action $a$ was taken. For each time step $i$, if $A_{i-1} = a$, $R_i$ is included in the sum; otherwise, it contributes 0.

- **Denominator: $\sum_{i=1}^{n} \mathbb{I}\{A_{i-1} = a\}$**: This counts the number of times action $a$ has been selected in the first $n$ time steps.

Thus, $q_n(a)$ is the average reward received from action $a$ up to the $n^{th}$ time step, continuously updated as more actions are taken and rewards are received.

With these action-value estimates, we can formulate a policy to decide which action to take at each step, balancing exploration and exploitation to optimize long-term rewards.


### Greedy Policy

- **Description**: The greedy policy is an action selection strategy where the agent consistently chooses the action with the highest estimated value based on current knowledge.

- **Formal Definition**: The action $a$ is selected such that it maximizes the estimated action-value $q_n(b)$ over all possible actions $b \in \{1, 2, \ldots, k\}$:

  $$ 
  a = \arg\max_{b} q_n(b) \quad \text{for } b \in \{1, 2, \ldots, k\} 
  $$

- **Explanation**: Intuitively, the agent evaluates its current estimates of the value of each action, denoted as $q_n(1), q_n(2), \ldots, q_n(k)$, and selects the action with the highest value. This approach is termed "greedy" because it focuses on the immediate best option, potentially overlooking exploration of other actions.

- **Limitations**:
  - **Lack of Exploration**: A purely greedy policy does not explore other actions. If an action is initially perceived as slightly better due to random fluctuations or limited data, the agent will repeatedly choose it, missing out on potentially better actions.
  - **Suboptimal Solutions**: In the context of the multi-armed bandit problem, especially early in the learning process, the action-value estimates $q_n(a)$ may be inaccurate. A greedy policy can lead to premature convergence on a suboptimal action if it is initially overestimated, preventing the discovery of superior actions.

- **When to Use**: The greedy policy is generally unsuitable for scenarios where exploration is crucial, such as in the early stages of reinforcement learning. It may be appropriate when initial estimates are highly reliable or when the focus is on exploitation rather than exploration.

### $\epsilon$-Greedy Policy

- **Description**: The $\epsilon$-greedy policy is a strategy designed to balance the trade-off between exploration (trying new actions) and exploitation (choosing the best-known action). It predominantly exploits the best-known action but occasionally explores other actions to improve knowledge.

- **Mechanism**: The policy operates by selecting the action with the highest estimated value with a probability of $1-\epsilon$, and with a probability of $\epsilon$, it selects an action randomly from the set of all possible actions.

- **Formal Definition**:
  
  - **With probability $1-\epsilon$ (Exploitation)**: Select an action that has the maximum estimated action-value. If there are multiple such actions, any one of them can be chosen. Mathematically, the selected action $a$ belongs to the set of actions that maximize $q_n(b)$:
    $$ a \in \arg\max_{b \in \{1, 2, \ldots, k\}} q_n(b) $$

  - **With probability $\epsilon$ (Exploration)**: Select an action randomly from the set of all possible actions, with each action having an equal probability of being selected. Mathematically, for each action $b \in \{1, 2, \ldots, k\}$, the probability of selecting $b$ is $\frac{1}{k}$:
    $$ P(a = b) = \frac{1}{k} \quad \text{for } b \in \{1, 2, \ldots, k\} $$

- **Role of $\epsilon$**:
  
  - $\epsilon$ is a hyperparameter that determines the level of exploration. It ranges from 0 to 1.
  - **High $\epsilon$ (close to 1)**: Encourages more exploration, allowing the agent to discover potentially better actions, though it may slow convergence to the optimal policy.
  - **Low $\epsilon$ (close to 0)**: Favors exploitation, leading to faster convergence if initial estimates are accurate, but risks missing out on better actions due to limited exploration.

- **Hyperparameter Tuning**: The choice of $\epsilon$ is crucial and often requires tuning. A common practice is to start with a higher $\epsilon$ (e.g., 0.1 or 0.2) to promote exploration initially and then reduce it over time.

- **Decaying $\epsilon$**: As the agent's knowledge improves, the need for exploration decreases. Thus, $\epsilon$ can be decayed over time, either linearly or exponentially, to allow more exploitation as confidence in action-value estimates grows.

- **Advantages**:
  
  - **Balances Exploration and Exploitation**: Offers a straightforward method to explore the action space while exploiting the best-known actions.
  - **Enhanced Long-term Performance**: By incorporating exploration, the $\epsilon$-greedy policy is more likely to identify the optimal action, improving long-term rewards compared to a purely greedy approach.

- **Considerations**:
  
  - **Choice of $\epsilon$**: The effectiveness of $\epsilon$ (and its decay schedule) can vary depending on the problem and may require experimentation.
  - **Uniform Random Action Selection**: While the policy selects actions uniformly at random during exploration, more advanced strategies might prioritize actions with higher uncertainty or less frequent selection. Despite this, the simplicity of uniform random selection makes $\epsilon$-greedy a popular and effective baseline strategy.


### Incremental Implementation for Efficient Action-Value Updates

- Action-value methods rely on estimating action values as the average of observed rewards.  A naive approach of storing all rewards and recalculating the average each time is computationally expensive and memory-intensive, especially as the number of rewards grows. To address this, we can use an incremental update approach that allows for efficient computation with constant memory and per-time-step computation.

- Let $q_n$ be the current estimate of an action's value after it has been selected $n-1$ times, and let $R_n$ be the $n^{th}$ reward received after selecting that action.  Instead of recalculating the sum of all rewards each time, we can update the action-value estimate $q_n$ to $q_{n+1}$ using the new reward $R_n$ and the previous estimate $q_n$.

- The incremental update formula is derived from the definition of the average:

  $$
  q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i
  $$

  This can be rewritten incrementally as (derivation skipped):

  $$
  q_{n+1} = q_n + \frac{1}{n} (R_n - q_n)
  $$

  where:
    - $q_{n+1}$ is the updated action-value estimate after observing the $n^{th}$ reward.
    - $q_n$ is the previous action-value estimate.
    - $R_n$ is the $n^{th}$ reward received.
    - $\frac{1}{n}$ acts as a step size, determining how much the new reward influences the old estimate.

- **Intuition**: This update rule adjusts the old estimate $q_n$ towards the new reward $R_n$. The term $(R_n - q_n)$ represents the error or difference between the new reward and the current estimate. We move a fraction of this error, determined by the step size $\frac{1}{n}$, to refine our estimate.  As $n$ increases, the step size $\frac{1}{n}$ decreases, meaning that newer rewards have a smaller impact on the estimate, giving more weight to the accumulated history of rewards.


