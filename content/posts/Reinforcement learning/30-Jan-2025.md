---
title: "Understanding Stochastic Shortest Path Problems: A Deep Dive into Mathematical Foundations"
date:
draft: false
description:
tags: []
categories: []
author:
toc:
weight: 1
---

## Definitions

### Expected Single-Stage Cost

The expected single-stage cost under policy $\mu$ in state $x$, denoted as $\bar{g}(x,\mu(x))$, is:

$$
\bar{g}(x,\mu(x)) = \sum_{j=0}^n p_{xj}(\mu(x))g(x,\mu(x),j)
$$

where:
- $p_{xj}(\mu(x))$ is the probability of transitioning from state $x$ to state $j$ under action $\mu(x)$
- $g(x,\mu(x),j)$ is the stage cost for this transition
- The sum is over all possible next states $j$

This represents the immediate expected cost when taking the action prescribed by policy $\mu$ in state $x$.


### Cost-to-Go Function for a Given Policy

For a given stationary policy $\mu$, the cost-to-go function $J_\mu(x)$ represents the expected total cost incurred when starting from state $x$ and following policy $\mu$ until reaching the terminal state. It can be expressed recursively as:

$$
J_\mu(x) = \bar{g}(x,\mu(x)) + \sum_{j=0}^n p_{xj}(\mu(x))J_\mu(j)
$$

where:
- $\bar{g}(x,\mu(x))$ is the expected single-stage cost defined above
- $p_{xj}(\mu(x))$ is the transition probability from state $x$ to state $j$ under action $\mu(x)$
- The sum represents the expected future costs from the next state onwards
- For the terminal state: $J_\mu(0) = 0$

### Bellman Operator for a Given Policy

The Bellman operator $T_\mu$ for a given policy $\mu$ maps a cost function $J$ to a new cost function $T_\mu J$ according to:

$$
(T_\mu J)(x) = \bar{g}(x,\mu(x)) + \sum_{j=0}^n p_{xj}(\mu(x))J(j)
$$

This operator represents one step of policy evaluation, taking into account both immediate costs and discounted future costs under policy $\mu$.

### Optimal Bellman Operator

The optimal Bellman operator $T$ maps a cost function $J$ to a new cost function $TJ$ by minimizing over all possible actions:
<div class="math-katex">
$$(TJ)(x) = \min_{a \in \mathcal{A}(x)} \left\{ \bar{g}(x,a) + \sum_{j=0}^n p_{xj}(a)J(j) \right\}$$
</div>

This operator represents one step of value iteration, finding the best action at each state given the current value estimates.



### Transition Probability Matrix $P_\mu$

For a given policy $\mu$, the transition probability matrix $P_\mu$ is an $n \times n$ square matrix where element $(i,j)$ represents the probability of transitioning from state $i$ to state $j$ under policy $\mu$. Since state 0 is the terminal state, we only include states 1 through $n$ in the matrix:

<div class="math-katex">
$$
P_\mu = \begin{pmatrix} 
p_{11}(\mu(1)) & p_{12}(\mu(1)) & \cdots & p_{1n}(\mu(1)) \\
p_{21}(\mu(2)) & p_{22}(\mu(2)) & \cdots & p_{2n}(\mu(2)) \\
\vdots & \vdots & \ddots & \vdots \\
p_{n1}(\mu(n)) & p_{n2}(\mu(n)) & \cdots & p_{nn}(\mu(n))
\end{pmatrix}
$$
</div>

Note that for each row $i$:
$$
\sum_{j=0}^n p_{ij}(\mu(i)) \leq 1, \quad \forall i
$$

This is because the sum of the probabilities of transitioning from state $i$ to all other states plus the probability of transitioning to the terminal state must equal 1.

### Expected Stage Cost Vector $\bar{g}_\mu$

The expected stage cost vector $\bar{g}_\mu$ is an $n$-dimensional column vector where element $i$ represents the expected single-stage cost for non-terminal state $i$ under policy $\mu$:

$$
\bar{g}_\mu = \begin{pmatrix}
\bar{g}(1,\mu(1)) \\
\bar{g}(2,\mu(2)) \\
\vdots \\
\bar{g}(n,\mu(n))
\end{pmatrix}
$$

### Matrix Notation for Bellman Operators
Using these matrix notations, we can express the Bellman operator for policy $\mu$ as:

$$
T_\mu J = \bar{g}_\mu + P_\mu J
$$

And for $k \geq 0$, the $k$-fold composition of the Bellman operator $T$ is:

$$
T^k = T(T^{k-1}J), \quad \text{where } T^0 = I
$$

This matrix notation provides a compact representation of the SSP problem and facilitates the analysis of solution algorithms.

### Interpretation of $T^k$ Operator

For $k=2$, the two-stage Bellman operator $T^2J(i)$ can be written as:

$$
T^2J(i) = T(TJ)(i) = \min_{u \in \mathcal{A}(i)} \left(\bar{g}(i,u) + \sum_{j=1}^n p_{ij}(u)TJ(j)\right)
$$

This expands to:

$$
T^2J(i) = \min_{u \in \mathcal{A}(i)} \left(\bar{g}(i,u) + \sum_{j=1}^n p_{ij}(u) \min_{u \in \mathcal{A}(j)} \left(\bar{g}(j,u) + \sum_{k=1}^n p_{jk}(u)J(k)\right)\right)
$$

**Interpretation:**
- $T^2J(i)$ represents the optimal cost for a 2-stage problem starting in state $i$
- It considers both:
  1. The immediate stage cost $\bar{g}(i,\cdot)$
  2. The terminal cost $J(\cdot)$

More generally, $T^kJ(i)$ represents the optimal cost of a $k$-stage problem with:
- Initial state $i$
- Stage cost function $\bar{g}(i,\cdot)$
- Terminal cost function $J(\cdot)$

Mathematically:

$$
T^kJ(i) = \min_{u \in \mathcal{A}(i)} \left(\bar{g}(i,u) + \sum_{j \in \mathcal{X}} p_{ij}(u)T^{k-1}J(j)\right), \quad \forall i=1,\ldots,n
$$

## Lemmas
### Lemma 1
For any $J,\bar{J} \in \mathbb{R}^n$ with $J(i) \leq \bar{J}(i)$ $\forall i=1,\ldots,n$ and any stationary policy $\mu$:

1. $T^kJ(i) \leq T^k\bar{J}(i)$ $\forall k \geq 0$, $\forall i=1,\ldots,n$
2. $T_\mu^kJ(i) \leq T_\mu^k\bar{J}(i)$ $\forall k \geq 0$, $\forall i=1,\ldots,n$

**Intuition:**
This lemma establishes the monotonicity property of Bellman operators. If we start with two value functions where one dominates the other ($J \leq \bar{J}$), applying the Bellman operator $k$ times preserves this ordering. 

This property is crucial for proving convergence of value iteration and policy iteration algorithms.

### Lemma 2
For any $k \geq 0$, vector $J \in \mathbb{R}^n$, stationary policy $\mu$, and $e=(1,1,\ldots,1)^T \in \mathbb{R}^n$:

1. $(T^k(J+ve))(i) \leq (T^kJ)(i) + v$ $\forall i=1,\ldots,n$
2. $(T_\mu^k(J+ve))(i) \leq (T_\mu^kJ)(i) + v$ $\forall i=1,\ldots,n$

where $v \geq 0$ is a scalar. The inequality is reversed if $v < 0$.

**Intuition:**
This lemma describes how the Bellman operators behave when we add a constant value $v$ to all states. It shows that:

- If we add a positive constant $v$ to all terminal costs (J+ve)
- The resulting k-stage optimal costs will increase by at most $v$
- This holds for both the optimal operator $T$ and policy-specific operator $T_\mu$

This property helps establish bounds on how value functions change when costs are perturbed uniformly across states.

## Assumptions
### Assumption 1: Existence of Proper Policy
There exists at least one proper policy. A proper policy is one that reaches the terminal state with probability 1 from any initial state.

**Mathematical Formulation:**
$\exists \mu$ such that starting from any non-terminal state $i$, following $\mu$ leads to the terminal state with probability 1.

**Importance:**
- Ensures the problem is well-defined
- Guarantees existence of finite optimal costs
- Without this, some policies might never reach the goal

### Assumption 2: Infinite Cost for Improper Policies
For any improper policy $\mu$ and any state $i$ from which the terminal state is not reached with probability 1, the cost-to-go is infinite:

$$
J_\mu(i) = \infty \quad \text{if } \mu \text{ is improper at state } i
$$

**Importance:**
- Penalizes policies that might get stuck in cycles
- Ensures optimal policy will be proper
- Forces convergence to terminal state

These assumptions together ensure:
1. Problem has a meaningful solution (Assumption 1)
2. Solution will reach the goal state (Assumption 2)
3. Optimal costs are well-defined and finite

## Propositions
### Proposition 1
For any proper policy $\mu$, the associated cost vector $J_\mu$ satisfies:

1. $J_\mu = \lim_{k \to \infty} T_\mu^k J$ for any $J \in \mathbb{R}^n$
2. $J_\mu$ is the unique solution to the equation $J_\mu = T_\mu J_\mu$

**Intuition:**
This proposition establishes two key properties of the cost vector $J_\mu$ for proper policies:

1. It can be obtained through repeated application of the Bellman operator $T_\mu$
2. It uniquely satisfies the Bellman equation for policy $\mu$

These properties are fundamental for:
- Proving convergence of policy evaluation
- Establishing uniqueness of cost-to-go values
- Justifying iterative methods for computing $J_\mu$

The uniqueness property is particularly important as it ensures that policy evaluation has a well-defined solution.





