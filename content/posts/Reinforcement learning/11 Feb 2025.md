---
title: "Infinite horizon discounted cost problems"
date:
draft: false
description:
tags: []
categories: []
author:
toc:
weight: 1
---

## Infinite horizon discounted cost problems

In infinite horizon discounted cost problems, we consider scenarios without a termination state and an infinite sequence of decisions. Let's understand the key components and mathematical formulation.

### Problem Setting

- **State Space**: Set of states $S = \{1,2,...,n\}$
- **Action Space**: For each state $i$, set of feasible actions $A(i)$
  - Total action space: $A = \bigcup_{i \in S} A(i)$
- **Cardinality**: Both $|S|$ and $|A|$ are finite

### Optimal Cost Function

The optimal cost function $J^*(i)$ represents the minimum total discounted cost starting from state $i$:

$$
\underbrace{J^*(i)}_{\substack{\text{Optimal cost} \\ \text{from state } i}} = \min_{u} \left[ \sum_{k=0}^{\infty} \alpha^k g(i_k,\mu(i_k),i_{k+1}) \mid i_0 = i \right]
$$

where:

- $\alpha \in (0,1)$ is the discount factor
- $g(i,u,j)$ is the stage cost for transition from state $i$ to $j$ under action $u$
- $i_k$ represents the state at stage $k$
- $\mu(i_k)$ is the policy decision at stage $k$

### Bellman Operators

There are two key Bellman operators in infinite horizon problems:

1. **Optimal Bellman Operator** $T$ (maps any value function $J$ to another value function):

$$
\underbrace{(TJ)(i)}_{\substack{\text{New value function} \\ \text{at state } i}} = \underbrace{\min_{u \in A(i)}}_{\substack{\text{Minimization} \\ \text{over actions}}} \underbrace{\sum_{j \in S} p_{ij}(u)(g(i,u,j) + \alpha J(j))}_{\substack{\text{Expected cost: immediate cost} \\ \text{plus discounted future cost}}}, \quad i \in S
$$

2. **Policy Bellman Operator** $T_{\mu}$ (specific to policy $\mu$):

$$
\underbrace{(T_{\mu}J)(i)}_{\substack{\text{New value function} \\ \text{at state } i}} = \underbrace{\sum_{j \in S} p_{ij}(\mu(i))(g(i,\mu(i),j) + \alpha J(j))}_{\substack{\text{Expected cost under policy } \mu: \\ \text{immediate cost plus discounted future cost}}}, \quad i \in S
$$

where:

- $p_{ij}(u)$ is the transition probability from state $i$ to $j$ under action $u$
- $J$ is any value function
- $\alpha$ is the discount factor that weighs future costs less than immediate costs
- $\mu$ is a specific policy mapping states to actions

### Matrix Notation

The policy Bellman operator can be expressed in matrix form for computational convenience:

Let's break down the components:

1. **Transition Probability Matrix** $P_{\mu}$:

   $$
   P_{\mu} = \begin{bmatrix}
   p_{11}(\mu(1)) & p_{12}(\mu(1)) & \cdots & p_{1n}(\mu(1)) \\
   p_{21}(\mu(2)) & p_{22}(\mu(2)) & \cdots & p_{2n}(\mu(2)) \\
   \vdots & \vdots & \ddots & \vdots \\
   p_{n1}(\mu(n)) & p_{n2}(\mu(n)) & \cdots & p_{nn}(\mu(n))
   \end{bmatrix}
   $$

   where:

   - Each row sums to 1: $\sum_{j \in S} p_{ij}(\mu(i)) = 1$ for all $i \in S$
   - No terminal states in infinite horizon problems

2. **Stage Cost Vector** $g_{\mu}$:
   $$
   g_{\mu} = \begin{bmatrix}
   \sum_{j \in S} p_{1j}(\mu(1))g(1,\mu(1),j) \\
   \sum_{j \in S} p_{2j}(\mu(2))g(2,\mu(2),j) \\
   \vdots \\
   \sum_{j \in S} p_{nj}(\mu(n))g(n,\mu(n),j)
   \end{bmatrix}
   $$

### Bellman Equation Under Policy $\mu$

The Bellman equation for a given policy $\mu$ can be written in matrix form:

$$
T_{\mu}J = g_{\mu} + \alpha P_{\mu}J
$$

This compact notation helps in:

- Understanding the structure of the problem
- Implementing efficient computational methods
- Analyzing convergence properties of solution algorithms

### Monotonicity Lemma

For any vectors $J, \bar{J} \in \mathbb{R}^n$ such that $J(i) \leq \bar{J}(i)$ for all $i \in S$ and for any stationary policy $\mu$, the following monotonicity properties hold:

1. For the optimal Bellman operator $T$:

   $$
   (T^kJ)(i) \leq (T^k\bar{J})(i) \quad \forall i \in S, k=1,2,\ldots
   $$

2. For the policy Bellman operator $T_\mu$:
   $$
   (T_\mu^kJ)(i) \leq (T_\mu^k\bar{J})(i) \quad \forall i \in S, k=1,2,\ldots
   $$

This lemma establishes that:

- Both Bellman operators preserve ordering between value functions
- If we start with a lower value function, applying the operators repeatedly will still result in lower values
- The monotonicity property is crucial for proving convergence of value iteration and policy iteration algorithms

### Linearity Lemma

For any vectors $J, r \in \mathbb{R}^n$, scalar $\alpha$, and stationary policy $\mu$, where $e$ is a vector of all ones ($e = [1,1,\ldots,1]^T \in \mathbb{R}^n$), the following linearity properties hold:

1. For the optimal Bellman operator $T$:

   $$
   (T^k(J+re))(i) = (T^kJ)(i) + \alpha^k r \quad \forall i \in \{1,2,\ldots,n\}, k \geq 1
   $$

2. For the policy Bellman operator $T_\mu$:
   $$
   (T_\mu^k(J+re))(i) = (T_\mu^kJ)(i) + \alpha^k r \quad \forall i \in \{1,2,\ldots,n\}, k \geq 1
   $$

This lemma establishes that:

- The Bellman operators exhibit linear behavior with respect to constant shifts
- Adding a constant to the value function results in a scaled addition to the operated value
- The scaling factor $\alpha^k$ reflects the discount factor raised to the power of iterations
- This property is useful in analyzing convergence and error bounds of dynamic programming algorithms

The linearity property complements the monotonicity lemma and helps in:

- Understanding how value functions change under constant shifts
- Analyzing the behavior of value iteration and policy iteration
- Deriving error bounds and convergence rates
- Simplifying theoretical analysis of dynamic programming algorithms

### Converting Discounted Cost Problems(DCP) to Stochastic Shortest Path Problems(SSPP) by adding a terminal state

![](5.png)
Let's understand how we can convert a discounted cost problem (DCP) into a stochastic shortest path problem (SSPP) by adding a terminal state.

### Key Idea

The main concept is to introduce an artificial terminal state that captures the discounting effect through transition probabilities.

### Mathematical Formulation

For a DCP with discount factor $\alpha$, we can construct an equivalent SSPP where:

1. **Terminal State Properties**:

   - Add a terminal state $t$ to the state space
   - Set $g(i,u,t) = 0$ for all states $i$ and controls $u$
   - Make state $t$ cost-free and absorbing: $g(t,u,t) = 0$

2. **Transition Probabilities**:
   For any state $i$ and control $u$:

   $$
   \underbrace{P(\text{termination in }k\text{th stage})}_{\text{Probability of reaching terminal state}} = \begin{cases}
   1-\alpha & \text{for }k=1 \\
   \alpha(1-\alpha) & \text{for }k=2 \\
   \alpha^{k-1}(1-\alpha) & \text{for }k\text{th stage}
   \end{cases}
   $$

3. **Non-termination Probability**:
   The probability of not terminating by stage $k$ is:

   $$
   \begin{align*}
   P(\text{non-termination by stage }k) &= 1-\{(1-\alpha) + \alpha(1-\alpha) + ... + \alpha^{k-1}(1-\alpha)\} \\
   &= 1-\frac{(1-\alpha)(1-\alpha^k)}{1-\alpha} = \alpha^k
   \end{align*}
   $$

4. **Expected Stage Cost**:
   For the $k$th stage:
   $$
   \underbrace{\text{Expected single stage cost}}_{\text{In }k\text{th stage}} = \alpha^k \sum_{j=1}^n p_{ij}(u)g(i,u,j)
   $$
   where $g(i,u,t) = 0$ for the terminal state $t$

### Proposition (DP Convergence)

For a discounted cost problem (DCP) with:

- Bounded stage costs $g(i,u,j) \leq M$ for all $i,j \in S$ and $u \in A(i)$
- Discount factor $\alpha < 1$

The optimal cost function $J^*$ satisfies:

$$
\underbrace{J^*(i)}_{\substack{\text{Optimal cost} \\ \text{starting from state }i}} = \lim_{N \to \infty} (T^NJ)(i) \quad \forall i \in S
$$

where:

- $T$ is the Bellman operator
- $T^N$ denotes N successive applications of $T$
- $J$ is any bounded function

### Corollary (Policy Convergence)

For any stationary policy $\mu$, the associated cost function satisfies:

$$
\underbrace{J_{\mu}(i)}_{\substack{\text{Cost function} \\ \text{for policy }\mu}} = \lim_{N \to \infty} (T_{\mu}^NJ)(i) \quad \forall i \in S
$$

where:

- $T_{\mu}$ is the policy Bellman operator
- $T_{\mu}^N$ denotes N successive applications of $T_{\mu}$
- $J$ is any bounded function
