---
title: "Score matching on 17 Oct 2024"
math: true
draft: false
---
# Class date - 17 Oct 2024
## Recap
![DDPM](4.jpg)

## Plan for today
![DDPM](5.jpg)

## Denoising score matching
Denoising score matching (DSM) is an alternative approach to score matching. It introduces an auxiliary variable and works with conditional scores, which offers several advantages.

Let's start with the DSM objective:

$$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x,x')} \left[ \| \hat{s}_\theta(x) - \nabla_{x} \log p(x|x') \|^2 \right]$$

Where:
- $x$ is the original data point
- $x'$ is the noisy version of $x$
- $\hat{s}_\theta(x)$ is the estimated score function
- $\nabla_{x} \log p(x|x')$ is the conditional score

Key aspects of Denoising Score Matching:

1. Auxiliary Variable: DSM introduces $x'$, which is a noisy version of the original data point $x$. This allows us to work with conditional distributions.

2. Conditional Score: Instead of estimating the score of the data distribution directly, DSM estimates the conditional score $\nabla_{x} \log p(x|x')$.

3. Gaussian Noise: Typically, Gaussian noise is added to create $x'$. This has two significant benefits:
   - We can add Gaussian noise to our data easily.
   - The conditional distribution $p(x|x')$ becomes Gaussian, which has a known analytical form for its score.

4. Theorem: There's an important relationship between DSM and the original score matching objective:

   $$J_{ESM}(\theta) = J_{DSM}(\theta) + C$$

   Where $C$ is a constant independent of $\theta$. This theorem shows that optimizing the DSM objective is equivalent to optimizing the original score matching objective.

The benefits of using conditional scores and Gaussian noise make DSM a powerful and practical approach to score matching, addressing many of the computational challenges faced by earlier methods.


## Continues DSM

The DSM objective was:

$$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x,x')} \left[ \| \hat{s}_\theta(x) - \nabla_{x} \log p(x|x') \|^2 \right]$$

We change the notation to match the lecture ie:
- The original datapoint $x$ is now $\tilde{x}$
- The noisy version $x'$ is now $x$

$$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(\tilde{x},x)} \left[ \| \hat{s}_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log p(\tilde{x}|x) \|^2 \right]$$



$$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \|^2 \right]$$

Here:
- $q_\sigma(\tilde{x}|x)$ is the perturbation distribution, defined as $q_\sigma(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2I)$, where $\tilde{x} = x + \sigma\epsilon$ and $\epsilon \sim \mathcal{N}(0, I)$. This perturbation kernel adds Gaussian noise to the original data point $x$ to create the noisy version $\tilde{x}$
- $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)$ is the score of the perturbation kernel. This can be derived as:

  $$\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) = -\nabla_{\tilde{x}} \frac{(\tilde{x}-x)^2}{2\sigma^2} = -\frac{\tilde{x}-x}{\sigma^2}$$

  Substituting this into the DSM objective, we get:

  $$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \| s_\theta(\tilde{x}) + \frac{\tilde{x}-x}{\sigma^2} \|^2 \right]$$


## Reparameterization

This formulation can be further simplified by reparameterizing $\tilde{x}$ in terms of $x$ and $\epsilon$:

  $$\tilde{x} = x + \sigma\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

Substituting this into our objective function:

$$J_{DSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)} \left[ \| s_\theta(x + \sigma\epsilon) + \frac{\epsilon}{\sigma} \|^2 \right]$$

Following the image more precisely, we can further simplify the objective function by taking the $\frac{1}{\sigma^2}$ out as a coefficient:

$$J_{DSM}(\theta) = \frac{1}{2\sigma^2} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)} \left[ \| \sigma s_\theta(x + \sigma\epsilon) + \epsilon \|^2 \right]$$

This was written in the lecture as(Honestly not very clear):

$$\mathbb{E}_{x,\epsilon} \left( \frac{1}{2\sigma^2} \| \epsilon_\theta(\tilde{x}) - \epsilon \|^2 \right)$$


Where:
1. $\epsilon_\theta(\tilde{x})$ represents the model's prediction of the noise added to the original data point.
2. $\tilde{x} = x + \sigma\epsilon$ is the noisy version of the original data point $x$.
3. $\epsilon \sim \mathcal{N}(0, I)$ is the actual noise added.

This formulation has several important properties:

1. Scaling factor: The $\frac{1}{2\sigma^2}$ term acts as a scaling factor, emphasizing accurate noise estimation at smaller noise levels.

2. Direct noise prediction: Instead of predicting the score, the model now directly predicts the noise $\epsilon_\theta(\tilde{x})$ that was added to create $\tilde{x}$.

3. Noise level adaptation: The model learns to predict noise across various scales, as $\sigma$ can vary.

4. Learning dynamics: This encourages the model to learn denoising at different noise levels, capturing both fine details and coarser structures.

5. Numerical stability: By directly predicting noise rather than scaled scores, this form can help prevent numerical instabilities when $\sigma$ is very small.

This representation of the DSM objective is particularly useful for score-based generative models, as it directly relates the learned function to the noise level, allowing for effective training across a range of perturbation magnitudes.

## Data coverage problem
The data coverage problem in score matching arises from the fact that we want the estimated score function $\hat{s}_\theta(x)$ to be accurate across the entire data distribution $p(x)$. However, in practice, we often face challenges in achieving this, especially in low-density regions of $p(x)$.

The data coverage problem can be addressed by using multiple noise scales, which is the key idea behind Noise Conditional Score Networks (NCSNs). Here's how this approach works:

1. Use a range of noise scales: 
   $$\{\sigma_i\}_{i=1}^L, \quad \text{where } \sigma_1 < \sigma_2 < ... < \sigma_L$$

   Here, $L$ is the number of noise scales, typically chosen to be a large number (e.g., 10 or more).

2. Define the noise scales such that:
   - $\sigma_1 = \sigma_{min}$ (smallest noise scale)
   - $\sigma_L = \sigma_{max}$ (largest noise scale)

3. The noise scales are chosen to satisfy two important conditions:
   - $p_{\sigma_{min}}(x) \approx p(x)$: The distribution of data with minimum noise added should be close to the true data distribution.
   - $p_{\sigma_{max}}(x) \approx \mathcal{N}(0, I)$: The distribution of data with maximum noise added should be close to a standard normal distribution.
4. Perturb the data according to each noise scale:
   $$p_{\sigma_i}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma_i^2I)$$

   Where $\tilde{x}$ is the perturbed data point, $x$ is the original data point, and $\sigma_i$ is the noise scale.

5. Train the model to estimate the score at each noise scale:
   $$s_\theta(x, \sigma_i) \approx \nabla_x \log p_{\sigma_i}(x)$$

   Where $s_\theta(x, \sigma_i)$ is the estimated score function conditioned on both the input $x$ and the noise scale $\sigma_i$.

6. The loss function for a single noise scale can be written as:
   $$\mathcal{L}(\theta, \sigma_i) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{q_{\sigma_i}(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}, \sigma_i) + \frac{\tilde{x}-x}{\sigma_i^2} \right\|^2 \right]$$

7. The overall training objective becomes:
   $$\mathcal{L}(\theta; \{\sigma_i\}_{i=1}^L) = \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \mathcal{L}(\theta, \sigma_i)$$

   Where $\lambda(\sigma_i)$ is a weighting function that can be used to emphasize certain noise scales.

This approach effectively addresses the data coverage problem by:

- Ensuring accurate score estimation in high-density regions (small $\sigma_i$)
- Providing stable training in low-density regions (large $\sigma_i$)
- Allowing the model to learn a smooth transition between different noise levels

By using multiple noise scales, Noise Conditional Score Networks (NCSNs) can effectively model the entire data distribution, including low-density regions, leading to improved generation quality and stability in score-based generative models.

![DDPM](6.jpg)










